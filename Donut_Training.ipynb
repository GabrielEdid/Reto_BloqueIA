{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba712c3",
   "metadata": {},
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Normalize\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from typing import Callable, Optional, List, Dict\n",
    "\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "import evaluate\n",
    "\n",
    "# Configure device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e80c5ec",
   "metadata": {},
   "source": [
    "## Load CORD-v2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff52a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CORD-v2 dataset\n",
    "print(\"Loading CORD-v2 dataset from Hugging Face:\")\n",
    "ds = load_dataset(\"naver-clova-ix/cord-v2\")\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"Dataset structure:\")\n",
    "print(ds)\n",
    "\n",
    "# Show dataset splits\n",
    "print(\"Available splits:\")\n",
    "for split in ds.keys():\n",
    "    print(f\"    - {split}: {len(ds[split])} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a0d8a",
   "metadata": {},
   "source": [
    "## Custom Transform Classes\n",
    "\n",
    "Define preprocessing transforms used in the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLAHETransform:\n",
    "    \"\"\"Apply CLAHE to improve local contrast in images.\"\"\"\n",
    "    \n",
    "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clip_limit = clip_limit\n",
    "        self.tile_grid_size = tile_grid_size\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Convert PIL Image to numpy array\n",
    "        img_np = np.array(img)\n",
    "        \n",
    "        # Convert to LAB color space\n",
    "        lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n",
    "        \n",
    "        # Apply CLAHE to L channel\n",
    "        clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n",
    "        lab[:, :, 0] = clahe.apply(lab[:, :, 0])\n",
    "        \n",
    "        # Convert back to RGB\n",
    "        img_clahe = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "        \n",
    "        # Return as PIL Image\n",
    "        return Image.fromarray(img_clahe)\n",
    "\n",
    "\n",
    "class SharpenTransform:\n",
    "    \"\"\"Sharpen image to enhance text clarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, kernel_size=(5, 5), sigma=1.0, amount=1.5):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.sigma = sigma\n",
    "        self.amount = amount\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Convert PIL Image to numpy array\n",
    "        img_np = np.array(img)\n",
    "        \n",
    "        # Create blurred version\n",
    "        blurred = cv2.GaussianBlur(img_np, self.kernel_size, self.sigma)\n",
    "        \n",
    "        # Unsharp mask: original + amount * (original - blurred)\n",
    "        sharpened = cv2.addWeighted(img_np, 1.0 + self.amount, blurred, -self.amount, 0)\n",
    "        \n",
    "        # Clip values to valid range\n",
    "        sharpened = np.clip(sharpened, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Return as PIL Image\n",
    "        return Image.fromarray(sharpened)\n",
    "\n",
    "\n",
    "# Initialize transform instances\n",
    "clahe_transform = CLAHETransform(clip_limit=2.0, tile_grid_size=(8, 8))\n",
    "sharpen_transform = SharpenTransform(amount=1.0)\n",
    "\n",
    "print(\"Custom transforms defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa4aea",
   "metadata": {},
   "source": [
    "## Helper Functions for Donut\n",
    "\n",
    "Utility functions for processing ground truth annotations and generating structured text sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e229c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2token(obj, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
    "    \"\"\"\n",
    "    Convert JSON object to a token sequence for Donut.\n",
    "    \n",
    "    This function recursively traverses the JSON structure and converts it into a special\n",
    "    token format that Donut can process. Each key-value pair is wrapped in special tokens\n",
    "    that indicate the hierarchical structure.\n",
    "    \n",
    "    Args:\n",
    "        obj: JSON object (dict, list, or primitive)\n",
    "        update_special_tokens_for_json_key: Whether to wrap keys in special tokens\n",
    "        sort_json_key: Whether to sort dictionary keys alphabetically\n",
    "    \n",
    "    Returns:\n",
    "        str: Token sequence representation of the JSON\n",
    "    \"\"\"\n",
    "    if type(obj) == dict:\n",
    "        if len(obj) == 1 and \"text_sequence\" in obj:\n",
    "            return obj[\"text_sequence\"]\n",
    "        else:\n",
    "            output = \"\"\n",
    "            if sort_json_key:\n",
    "                keys = sorted(obj.keys(), reverse=True)\n",
    "            else:\n",
    "                keys = obj.keys()\n",
    "            for k in keys:\n",
    "                if update_special_tokens_for_json_key:\n",
    "                    output += (\n",
    "                        fr\"<s_{k}>\" + json2token(obj[k], update_special_tokens_for_json_key, sort_json_key) + fr\"</s_{k}>\"\n",
    "                    )\n",
    "                else:\n",
    "                    output += json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
    "            return output\n",
    "    elif type(obj) == list:\n",
    "        return r\"<sep/>\".join(\n",
    "            [json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
    "        )\n",
    "    else:\n",
    "        obj = str(obj)\n",
    "        if f\"<{obj}>\" in processor.tokenizer.all_special_tokens:\n",
    "            obj = fr\"\\{obj}\"  # escape special tokens\n",
    "        return obj\n",
    "\n",
    "\n",
    "def preprocess_documents_for_donut(sample):\n",
    "    \"\"\"\n",
    "    Preprocess a single sample for Donut training.\n",
    "    \n",
    "    This function prepares the image and ground truth JSON for Donut by:\n",
    "    1. Converting the image to RGB\n",
    "    2. Processing the image through the Donut processor\n",
    "    3. Converting the ground truth JSON to token sequence\n",
    "    4. Creating decoder input and labels\n",
    "    \n",
    "    Args:\n",
    "        sample: Dictionary containing 'image' and 'ground_truth' keys\n",
    "    \n",
    "    Returns:\n",
    "        dict: Processed sample with pixel_values, labels, and target_sequence\n",
    "    \"\"\"\n",
    "    # Convert image to RGB\n",
    "    image = sample['image'].convert('RGB')\n",
    "    \n",
    "    # Process image\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "    \n",
    "    # Process ground truth\n",
    "    task_prompt = \"<s_cord-v2>\"\n",
    "    ground_truth = sample['ground_truth']\n",
    "    target_sequence = task_prompt + json2token(ground_truth) + processor.tokenizer.eos_token\n",
    "    \n",
    "    # Tokenize\n",
    "    input_ids = processor.tokenizer(\n",
    "        target_sequence,\n",
    "        add_special_tokens=False,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )[\"input_ids\"].squeeze(0)\n",
    "    \n",
    "    return {\n",
    "        \"pixel_values\": pixel_values.squeeze(),\n",
    "        \"labels\": input_ids,\n",
    "        \"target_sequence\": target_sequence\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Donut helper functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1935fba",
   "metadata": {},
   "source": [
    "# Donut: Document Understanding Transformer for Structured Receipt Information Extraction\n",
    "---\n",
    "Donut (Document Understanding Transformer) is a vision-language model that specialized in extracting structured JSON outputs containing key-value pairs, which makes it good for parsing receipts into organized data formats. It direclty converts documents images into structured text, eliminating any kind of errors that might occur during the pipeline process on other systems. Extracting structured information from receipts is very challenging because they can have different layouts with different store formats and receipt designs, as well as other information, such as sotre details, iterm with prices, subtotals, and totals. Also, the model to extract the text needs to understand semantic differences that distinguishes between similar numbers, like item price vs total, missing fields, formatting variations, and more. Because of all this, using Donut is beneficial, since:\n",
    "- It is pre-trained on large-scale documents, which gives the ability to understand document structure from millions of images.\n",
    "- Generated structured JSON output directly, which is perfect for dtatabase storage and downstream applications.\n",
    "- It contains multi-modal undrstanding that combines visual layout and text semantics.\n",
    "- We can apply transfer learning to leverage knowledge from divese document types.\n",
    "\n",
    "Donut follows an encoder-decoder architecture where:\n",
    "\n",
    "1. **Encoder: Swin Transformer**:\n",
    "    - **Input**: Receipt an RGB image and resized to fixed dimensions.\n",
    "    - **Process**: The image is divided into patches using hierarchical windows, creating multi-scale feature representations\n",
    "    - **Output**: Sequence of visual embeddings representing document structure and content\n",
    "    - **Pre-trained on**: ImageNet-22K and document images\n",
    "    - **What it does**: It extracts visual features, while preserving spatial relationships and hierarchical document structure\n",
    "2. **Decoder: mBART (Multilingual BART)**:\n",
    "    - **Input**: Visual embeddings from encoder and task prompt `<s_cord-v2>`\n",
    "    - **Process**: Generates JSON tokens autoregressively (one character at a time)\n",
    "    - **Output**: Structured JSON string containing receipt information\n",
    "    - **Pre-trained on**: Large multilingual text corpora\n",
    "    - **What it does**: Converts visual features into structured JSON using sequence-to-sequence generation. It is very robust to receipt variations, such as different layouts, fonts, and quality.\n",
    "    - It uses mBART since it has a strong multilingual capabilities and can handle various receipt languages, robust JSON generation trained on structured text formats, and contextual understanding to maintain valid JSON structure throughout generation. It also gives a high accuracy on structured information extraction\n",
    "3. **Cross-Attention Mechanism**: the decoder attends to encoder outputs at each generation step, and applying fine-tuning allows us to: \n",
    "    - Focus on specific document regions when generating each JSON key-value pair\n",
    "    - Align visual features with semantic labels. An alternative is to use `naver-clova-ix/donut-base-finetuned-cord-v2`, which is already fine-tuned on CORD-v2, but we want to compare our custom fine-tuning strategies.\n",
    "    - Handle variable document layouts and field positions\n",
    "\n",
    "In the following steps, we are going to implement three fine-tuning strategies to adapt the pre-trained Donut model to receipt parsing:\n",
    "1. **Strategy 1: Frozen Encoder (Feature Extraction)**:\n",
    "    - **Freeze**: All encoder layers (Swin Transformer)\n",
    "    - **Train**: Only decoder layers (mBART)\n",
    "    - **Reason**: The encoder already knows how to extract visual features from documents, since it is pre-trained on document images. We only need to adapt the decoder to receipt-specific JSON structure and vocabulary.\n",
    "    - **Advantages**: Fast training, low memory usage, less chance of overfitting\n",
    "    - **Disadvantages**: Limited adaptation to receipt-specific visual patterns, such as thermal printing artifacts and variable quality.\n",
    "    - **Best for**: Small datasets and limited compute resources (Our dataset has 1000 receipts).\n",
    "\n",
    "2. **Strategy 2: Partial Unfreezing (Progressive Fine-Tuning)**:\n",
    "    - **Freeze**: First N-2 encoder layers\n",
    "    - **Train**: Last 2 encoder layers, as well as all decoder layers\n",
    "    - **Reason**: Lower Swin Transformer layers learn generic features, like edges and windows, while higher layers learn task-specific patterns. By unfreezing the last 2 layers, we allow the model to adapt to receipt-specific visual characteristics like tables and text regions.\n",
    "    - **Advantages**: Better domain adaptation than frozen encoder\n",
    "    - **Disadvantages**: Requires more memory and compute than Strategy 1\n",
    "    - **Best for**: Medium-sized datasets with some computational budget\n",
    "\n",
    "3. **Strategy 3: Full Fine-Tuning (End-to-End Training)**:\n",
    "    - **Freeze**: Nothing\n",
    "    - **Train**: All encoder and decoder layers\n",
    "    - **Reason**: Maximum adaptation to receipts. The model can learn receipt-specific visual features and JSON generation patterns simultaneously.\n",
    "    - **Advantages**: Best performance potential, due to full customization\n",
    "    - **Disadvantages**: Requires large dataset, high memory/compute, but has risk of overfitting\n",
    "    - **Best for**: Large datasets and sufficient compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a607b36",
   "metadata": {},
   "source": [
    "## Donut Dataset Preparation\n",
    "\n",
    "As we mentioned before, Donut is trained to perform structured information extraction from document images.\n",
    "- **Input**: Receipt image (PIL Image)\n",
    "- **Output**: Structured JSON string (key-value pairs representing receipt fields)\n",
    "- **Error handling**: If JSON parsing fails, return empty JSON object to prevent training crashes.\n",
    "\n",
    "**CORD-v2 Dataset Structure**:\n",
    "\n",
    "The CORD-v2 dataset provides:\n",
    "- **Images**: Receipt images in PNG format\n",
    "- **Ground truth**: JSON annotations with:\n",
    "  - `valid_line`: List of text lines in the receipt\n",
    "  - `words`: Individual words with bounding boxes and text content\n",
    "  - `category`: Semantic labels (menu.nm, menu.price, total.total_price, etc.)\n",
    "\n",
    "**Data Extraction Process**:\n",
    "\n",
    "We need to convert the structured JSON annotations into a format suitable for Donut's sequence-to-sequence generation. To achieve this, we will follow these steps:\n",
    "1. **Parse JSON**: Load the `ground_truth` string and parse it as JSON\n",
    "2. **Extract structured fields**: Iterate through `valid_line` and `words` to collect all text\n",
    "3. **Create JSON output**: Package extracted information as a JSON string with key-value pairs\n",
    "4. **Add task prompt**: Prepend `<s_cord-v2>` token to guide the decoder\n",
    "5. **Example**:\n",
    "   - Input JSON: `{\"valid_line\": [{\"words\": [{\"text\": \"TOTAL\"}, {\"text\": \"$45.99\"}]}]}`\n",
    "   - Output sequence: `<s_cord-v2>{\"text\": \"TOTAL $45.99\"}</s>`\n",
    "\n",
    "**Preprocessing Pipeline**:\n",
    "\n",
    "For each image, we will:\n",
    "1. Apply our custom transforms, which are minimal ColorJitter for subtle augmentation to avoid aggressive transforms that hurt OCR.\n",
    "2. Use Donut processor to handle resizing, normalization, and conversion to tensors internally.\n",
    "3. Apply tokenization to convert JSON strings into token IDs using mBART tokenizer.\n",
    "4. Put in padding or truncation sequences to a `max_length` of 512 tokens.\n",
    "\n",
    "Some special cases include:\n",
    "- **Task prompt**: `<s_cord-v2>` is added as a special token to signal receipt parsing task\n",
    "- **Padding tokens**: Replaced with `-100` in labels so they're ignored during loss computation\n",
    "- **Max length**: Set to 512 tokens to accommodate structured JSON while fitting in GPU memory\n",
    "\n",
    "The following class is created to combine CORD's structured annotations and Donut's expected input and output formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DonutDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Donut fine-tuning on CORD-v2.\n",
    "    Extracts structured information (JSON) from receipt images.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hf_dataset,\n",
    "        processor,\n",
    "        image_transform: Optional[Callable] = None,\n",
    "        max_length: int = 768,\n",
    "        task_prompt: str = \"<s_cord-v2>\",\n",
    "    ):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.processor = processor\n",
    "        self.image_transform = image_transform\n",
    "        self.max_length = max_length\n",
    "        self.task_prompt = task_prompt\n",
    "        \n",
    "        # Add special tokens to processor\n",
    "        self.processor.tokenizer.add_special_tokens({\"additional_special_tokens\": [task_prompt]})\n",
    "        \n",
    "        print(f\"DonutDataset initialized with {len(self.hf_dataset)} samples\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def extract_structured_info(self, ground_truth_str: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract structured information from CORD ground truth.\n",
    "        Returns a JSON string with key-value pairs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            gt_dict = json.loads(ground_truth_str)\n",
    "            \n",
    "            # Extract relevant fields from CORD dataset\n",
    "            structured_data = {}\n",
    "            \n",
    "            if 'valid_line' in gt_dict:\n",
    "                all_text = []\n",
    "                for line in gt_dict['valid_line']:\n",
    "                    if 'words' in line:\n",
    "                        line_text = []\n",
    "                        for word in line['words']:\n",
    "                            if 'text' in word:\n",
    "                                line_text.append(word['text'])\n",
    "                        if line_text:\n",
    "                            all_text.append(' '.join(line_text))\n",
    "                \n",
    "                structured_data['text'] = ' '.join(all_text)\n",
    "            \n",
    "            # We can extract more specific fields like:\n",
    "            # - store_name, date, total, items, etc.\n",
    "            # For now, we'll use the full text as a simple example\n",
    "            \n",
    "            return json.dumps(structured_data, ensure_ascii=False)\n",
    "        except:\n",
    "            return json.dumps({\"text\": \"\"})\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        sample = self.hf_dataset[idx]\n",
    "        \n",
    "        # Get image\n",
    "        image = sample['image']\n",
    "        \n",
    "        # Apply custom preprocessing (CLAHE + Sharpening)\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        \n",
    "        # Extract structured information\n",
    "        structured_json = self.extract_structured_info(sample['ground_truth'])\n",
    "        \n",
    "        # Create target sequence: <s_cord-v2> + JSON + </s>\n",
    "        target_sequence = f\"{self.task_prompt}{structured_json}</s>\"\n",
    "        \n",
    "        # Process image\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "        \n",
    "        # Tokenize target sequence\n",
    "        labels = self.processor.tokenizer(\n",
    "            target_sequence,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids.squeeze()\n",
    "        \n",
    "        # Replace padding token id with -100\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels,\n",
    "            \"target_sequence\": target_sequence,\n",
    "        }\n",
    "\n",
    "# Test Donut dataset\n",
    "print(\"Loading Donut processor...\")\n",
    "donut_processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "# Donut preprocessing: only CLAHE and Sharpening (no Normalize, no ToTensor)\n",
    "# The Donut processor handles conversion to tensor and normalization internally\n",
    "donut_preprocess = transforms.Compose([\n",
    "    clahe_transform,\n",
    "    sharpen_transform,\n",
    "])\n",
    "\n",
    "# Create test dataset\n",
    "donut_test_dataset = DonutDataset(\n",
    "    hf_dataset=ds['train'],\n",
    "    processor=donut_processor,\n",
    "    image_transform=donut_preprocess,\n",
    "    max_length=768,\n",
    ")\n",
    "\n",
    "# Test sample\n",
    "test_sample = donut_test_dataset[0]\n",
    "print(f\"Sample output:\")\n",
    "print(f\"    - Pixel values shape: {test_sample['pixel_values'].shape}\")\n",
    "print(f\"    - Labels shape: {test_sample['labels'].shape}\")\n",
    "print(f\"    - Target sequence preview: {test_sample['target_sequence'][:150]}...\")\n",
    "print(f\"Donut Dataset ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0578dd35",
   "metadata": {},
   "source": [
    "## Donut Model Implementation with PyTorch Lightning\n",
    "\n",
    "We will use PyTorch Lightning for the same reasons as TrOCR:\n",
    "- **Organized code**: Separates research code from engineering code\n",
    "- **Built-in features**: Automatic logging, checkpointing, early stopping, multi-GPU support\n",
    "- **Reproducibility**: Handles random seeds, deterministic training\n",
    "- **Less boilerplate**: No need to manually write training loops, GPU transfer logic\n",
    "\n",
    "**Model Class Structure**: The following implementation of our `DonutLightningModel` inherits from `LightningModule` and implements:\n",
    "1. Initialization (`__init__`):\n",
    "    - Load pre-trained `VisionEncoderDecoderModel` (Swin + mBART) from Hugging Face\n",
    "    - Load corresponding `DonutProcessor` to handle image preprocessing and tokenization.\n",
    "    - Add task-specific token `<s_cord-v2>` to vocabulary and resize decoder embeddings\n",
    "    - Configure generation parameters, like start token, padding, and EOS token.\n",
    "    - Apply freezing strategy (freeze/unfreeze layers based on strategy)\n",
    "    - Initialize metrics: token-level accuracy\n",
    "2. Freezing Strategy (`_apply_freezing_strategy`):\n",
    "    - Iterate through `model.encoder.parameters()` and set `requires_grad = False` to freeze\n",
    "    - For partial unfreezing: Access Swin layers via `model.encoder.encoder.layers[-N:]` and unfreeze\n",
    "    - Decoder is always trainable: `model.decoder.parameters()` have `requires_grad = True`\n",
    "    - We freeze parameters, since those don't compute gradients, which is equivalent to faster training and less memory. Also, pre-trained weights are preserved and only task-specific layers adapt for better generalization on small datasets.\n",
    "3. Forward Pass (`forward`):\n",
    "    - Takes `pixel_values` (image tensors) and `labels` (JSON token IDs)\n",
    "    - Returns model outputs including loss and logits\n",
    "    - Loss is automatically computed by comparing logits with labels (cross-entropy)\n",
    "4. Training Step (`training_step`):\n",
    "    - This is what happens in each training iteration:\n",
    "        1. Forward pass: `outputs = self(pixel_values, labels=labels)`\n",
    "        2. Extract loss: `loss = outputs.loss`\n",
    "        3. Compute token-level accuracy from logits (no generation during training for efficiency)\n",
    "        4. Log metrics: `self.log('train_loss', loss)`, `self.log('train_acc', self.train_acc)`\n",
    "        5. Return loss (Lightning automatically calls `loss.backward()` and optimizer step)\n",
    "    - Computing accuracy from logits is much faster than generating full JSON sequences\n",
    "5. Validation Step (`validation_step`):\n",
    "    - Evaluate model on validation set to monitor overfitting\n",
    "        1. Forward pass, same as training.\n",
    "        2. Compute token-level accuracy from logits\n",
    "        3. Log metrics: `self.log('val_loss', loss)`, `self.log('val_acc', self.val_acc)`\n",
    "    - No JSON generation during validation for speed (only during test)\n",
    "6. Test Step (`test_step`):\n",
    "    - Generate full JSON sequences for qualitative evaluation\n",
    "        1. Forward pass for loss and accuracy\n",
    "        2. Generate predictions: `model.generate(pixel_values, max_length=512)`\n",
    "        3. Decode predictions to JSON strings\n",
    "        4. Compute token-level accuracy\n",
    "        5. Log metrics and return predictions for analysis\n",
    "7. Optimizer Configuration (`configure_optimizers`)\n",
    "    - AdamW Optimizer:\n",
    "        - Variant of Adam with decoupled weight decay to prevent overfitting\n",
    "        - Learning rate: 5e-5, which is commonly used for fine-tuning document understanding models\n",
    "        - Weight decay: 0.01, which is L2 regularization.\n",
    "    - Cosine Annealing with Warmup:\n",
    "        - Gradually increases LR during warmup (500 steps)\n",
    "        - Then decreases following cosine schedule\n",
    "        - Helps model converge smoothly and avoid early training instability\n",
    "\n",
    "Some key variables are:\n",
    "1. `max_length=512`: limit inference time and memory usage while accommodating structured JSON outputs. We decided on 512, since JSON outputs are typically longer than plain text.\n",
    "2. We replace padding with `-100` in labels, since PyTorch's `CrossEntropyLoss` ignores index `-100`, which ensures padding tokens don't contribute to loss and prevents the model from learning to predict padding.\n",
    "3. We have log metrics with `prog_bar=True` to display metrics in real-time during training, which helps us monitor training progress.\n",
    "4. `<s_cord-v2>` task prompt: guides the decoder to generate receipt-specific JSON structure\n",
    "\n",
    "This implementation follows best practices for fine-tuning vision-language models and provides a clean, maintainable codebase for experimentation with structured document understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b829ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DonutLightningModel(L.LightningModule):\n",
    "    \"\"\"\n",
    "    Donut model with PyTorch Lightning for document understanding.\n",
    "    \n",
    "    Fine-tuning strategy (same as TrOCR):\n",
    "    - Phase 1: Freeze encoder, train decoder only\n",
    "    - Phase 2: Unfreeze last encoder layers\n",
    "    - Phase 3: Full fine-tuning\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"naver-clova-ix/donut-base\",\n",
    "        learning_rate: float = 3e-5,\n",
    "        freeze_encoder: bool = True,\n",
    "        unfreeze_last_n_layers: int = 0,\n",
    "        max_length: int = 768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Load pre-trained Donut model\n",
    "        self.model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "        self.processor = DonutProcessor.from_pretrained(model_name)\n",
    "        \n",
    "        # Add task-specific token\n",
    "        self.processor.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<s_cord-v2>\"]})\n",
    "        self.model.decoder.resize_token_embeddings(len(self.processor.tokenizer))\n",
    "        \n",
    "        # Apply freezing strategy\n",
    "        self._apply_freezing_strategy(freeze_encoder, unfreeze_last_n_layers)\n",
    "        \n",
    "        # Configure generation parameters\n",
    "        self.model.config.decoder_start_token_id = self.processor.tokenizer.convert_tokens_to_ids([\"<s_cord-v2>\"])[0]\n",
    "        self.model.config.pad_token_id = self.processor.tokenizer.pad_token_id\n",
    "        self.model.config.eos_token_id = self.processor.tokenizer.eos_token_id\n",
    "        \n",
    "        # Metrics - Token-level accuracy (multiclass with ignore_index for padding)\n",
    "        vocab_size = len(self.processor.tokenizer)\n",
    "        self.train_acc = torchmetrics.Accuracy(\n",
    "            task=\"multiclass\",\n",
    "            num_classes=vocab_size,\n",
    "            ignore_index=-100\n",
    "        )\n",
    "        self.val_acc = torchmetrics.Accuracy(\n",
    "            task=\"multiclass\",\n",
    "            num_classes=vocab_size,\n",
    "            ignore_index=-100\n",
    "        )\n",
    "        \n",
    "    def _apply_freezing_strategy(self, freeze_encoder: bool, unfreeze_last_n_layers: int):\n",
    "        \"\"\"\n",
    "        Apply layer freezing strategy\n",
    "        \n",
    "        Args:\n",
    "            freeze_encoder: If True, freeze Swin Transformer encoder\n",
    "            unfreeze_last_n_layers: Number of last encoder layers to unfreeze\n",
    "        \"\"\"\n",
    "        if freeze_encoder:\n",
    "            # Freeze all encoder parameters\n",
    "            for param in self.model.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Encoder (Swin Transformer) frozen\")\n",
    "            \n",
    "            # Unfreeze last N layers if specified\n",
    "            if unfreeze_last_n_layers > 0:\n",
    "                # Access Swin Transformer layers\n",
    "                # Donut uses Swin Transformer which has a different structure\n",
    "                try:\n",
    "                    encoder_layers = self.model.encoder.encoder.layers\n",
    "                    for layer in encoder_layers[-unfreeze_last_n_layers:]:\n",
    "                        for param in layer.parameters():\n",
    "                            param.requires_grad = True\n",
    "                    print(f\"Unfroze last {unfreeze_last_n_layers} encoder layers\")\n",
    "                except:\n",
    "                    print(\"Could not unfreeze specific layers (model structure may vary)\")\n",
    "        else:\n",
    "            print(\"Encoder unfrozen (full fine-tuning mode)\")\n",
    "        \n",
    "        # Decoder is always trainable\n",
    "        for param in self.model.decoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Decoder (mBART) trainable\")\n",
    "        \n",
    "        # Count trainable parameters\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"Trainable parameters: {trainable_params:,} / {total_params:,} \"\n",
    "              f\"({100 * trainable_params / total_params:.2f}%)\")\n",
    "    \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        return self.model(pixel_values=pixel_values, labels=labels)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        outputs = self(pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Calculate token-level accuracy using torchmetrics\n",
    "        with torch.no_grad():\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Flatten for accuracy calculation\n",
    "            preds_flat = preds.view(-1)\n",
    "            labels_flat = labels.view(-1)\n",
    "            \n",
    "            # Update accuracy metric (ignore_index=-100 is already configured)\n",
    "            acc = self.train_acc(preds_flat, labels_flat)\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        outputs = self(pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Generate predictions\n",
    "        generated_ids = self.model.generate(\n",
    "            pixel_values,\n",
    "            max_length=self.hparams.max_length,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "        )\n",
    "        generated_texts = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Decode ground truth\n",
    "        labels_copy = labels.clone()\n",
    "        labels_copy[labels_copy == -100] = self.processor.tokenizer.pad_token_id\n",
    "        reference_texts = self.processor.batch_decode(labels_copy, skip_special_tokens=True)\n",
    "        \n",
    "        # Calculate token-level accuracy using torchmetrics\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        preds_flat = preds.view(-1)\n",
    "        labels_flat = labels.view(-1)\n",
    "        acc = self.val_acc(preds_flat, labels_flat)\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_acc', self.val_acc, prog_bar=True, on_epoch=True)\n",
    "        \n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        # Forward pass for accuracy calculation\n",
    "        outputs = self(pixel_values, labels=labels)\n",
    "        \n",
    "        # Generate predictions\n",
    "        generated_ids = self.model.generate(\n",
    "            pixel_values,\n",
    "            max_length=self.hparams.max_length,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "        )\n",
    "        generated_texts = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Calculate token-level accuracy using torchmetrics\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        preds_flat = preds.view(-1)\n",
    "        labels_flat = labels.view(-1)\n",
    "        \n",
    "        # Create a new accuracy metric for test (to avoid state issues)\n",
    "        test_acc = torchmetrics.Accuracy(\n",
    "            task=\"multiclass\",\n",
    "            num_classes=len(self.processor.tokenizer),\n",
    "            ignore_index=-100\n",
    "        ).to(self.device)\n",
    "        acc = test_acc(preds_flat, labels_flat)\n",
    "        \n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        \n",
    "        # Log or return predictions for analysis\n",
    "        return {'predictions': generated_texts, 'test_acc': acc}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=3\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss'\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"Donut Lightning Model defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffbd7bb",
   "metadata": {},
   "source": [
    "## Donut DataModule with Custom Preprocessing\n",
    "\n",
    "Now, we will implement a `LightningDataModule` to encapsulate all data-related logic in one place, such as:\n",
    "- Dataset creation (train/val/test splits)\n",
    "- Data transformations and augmentations\n",
    "- DataLoader configuration, like batch size, shuffling and workers.\n",
    "- Reproducibility to ensure consistent data handling across experiments.\n",
    "\n",
    "To not rely only on Donut processor, we use our custom preprocessing first:\n",
    "- **Minimal ColorJitter**: subtle brightness and contrast adjustments to avoid aggressive transforms that hurt OCR performance.\n",
    "- Applying first our custom transform is very important because it improves the image quality of the receipts without introducing artifacts. Then, using Donut Processor helps us resize, normalize and convert to tensor with the purpose of preparing for model input.\n",
    "\n",
    "**Augmentation Strategy**: \n",
    "- Training step: we will apply minimal augmentations to slightly increase dataset diversity.\n",
    "    - Color jitter with 0.05 brightness and contrast to simulate subtle lightning variations.\n",
    "    - No rotation or geometric transforms, since these can hurt document understanding performance.\n",
    "- Validation and Test sets: no augmentation, only processor normalization.\n",
    "    - We want to measure true performance on clean, processed images.\n",
    "    - Augmentation during validation would give misleading metrics.\n",
    "\n",
    "**DataLoader Configuration**:\n",
    "- **Batch size**: Set to 4 for frozen strategy, 2 for partial and full\n",
    "- **Shuffle**: True for training for random order, but False for validation and test\n",
    "- **num_workers**: Set to 4 for parallel data loading\n",
    "- **collate_fn**: Custom function to properly batch mixed data types (tensors and JSON strings)\n",
    "\n",
    "\n",
    "**Collate Function Explained**: The main issue is that PyTorch's default collate can't handle mixed types like tensors and list of strings. For this, we created a custom collate function that:\n",
    "1. Stacks `pixel_values` tensors into batch: `torch.stack([item['pixel_values'] for item in batch])`\n",
    "2. Stacks `labels` tensors into batch: `torch.stack([item['labels'] for item in batch])`\n",
    "3. Keeps target_sequence as list: `[item['target_sequence'] for item in batch]` (used for debugging and analysis)\n",
    "\n",
    "This will ensure us proper batching, while preserving JSON references for analysis and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75241125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DonutDataModule(L.LightningDataModule):\n",
    "    \"\"\"DataModule for Donut training with CORD-v2 dataset.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hf_dataset,\n",
    "        processor,\n",
    "        batch_size: int = 1,\n",
    "        num_workers: int = 0,\n",
    "        use_augmentation: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.processor = processor\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.use_augmentation = use_augmentation\n",
    "        \n",
    "        # Define preprocessing transforms (reusing our custom transforms)\n",
    "        self.train_transform = transforms.Compose([\n",
    "            CLAHETransform(clip_limit=2.0, tile_grid_size=(8, 8)),\n",
    "            SharpenTransform(amount=1.0),\n",
    "            transforms.RandomRotation(degrees=3, fill=255) if use_augmentation else transforms.Lambda(lambda x: x),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1) if use_augmentation else transforms.Lambda(lambda x: x),\n",
    "        ])\n",
    "        \n",
    "        self.val_transform = transforms.Compose([\n",
    "            CLAHETransform(clip_limit=2.0, tile_grid_size=(8, 8)),\n",
    "            SharpenTransform(amount=1.0),\n",
    "        ])\n",
    "    \n",
    "    def setup(self, stage: str = None):\n",
    "        self.train_dataset = DonutDataset(\n",
    "            hf_dataset=self.hf_dataset['train'],\n",
    "            processor=self.processor,\n",
    "            image_transform=self.train_transform,\n",
    "        )\n",
    "        \n",
    "        self.val_dataset = DonutDataset(\n",
    "            hf_dataset=self.hf_dataset['validation'],\n",
    "            processor=self.processor,\n",
    "            image_transform=self.val_transform,\n",
    "        )\n",
    "        \n",
    "        self.test_dataset = DonutDataset(\n",
    "            hf_dataset=self.hf_dataset['test'],\n",
    "            processor=self.processor,\n",
    "            image_transform=self.val_transform,\n",
    "        )\n",
    "        \n",
    "        print(f\"[DonutDataModule] Train: {len(self.train_dataset)}, \"\n",
    "              f\"Val: {len(self.val_dataset)}, Test: {len(self.test_dataset)}\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Custom collate function to handle batching.\n",
    "        \n",
    "        This is necessary to properly batch mixed data types: tensors (pixel_values, labels) \n",
    "        are stacked, while text strings are kept as a list.\n",
    "        \"\"\"\n",
    "        pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "        labels = torch.stack([item['labels'] for item in batch])\n",
    "        target_sequences = [item['target_sequence'] for item in batch]\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': labels,\n",
    "            'target_sequence': target_sequences,\n",
    "        }\n",
    "\n",
    "print(\"Donut DataModule defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831cbf63",
   "metadata": {},
   "source": [
    "## Donut Training Setup\n",
    "\n",
    "We train three independent Donut models using different fine-tuning strategies to compare performance vs computational cost.\n",
    "\n",
    "**Three Strategies**:\n",
    "\n",
    "| Strategy | Trainable Params | Frozen Params | Epochs | Batch Size | Use Case |\n",
    "|----------|-----------------|---------------|--------|------------|----------|\n",
    "| **1. Frozen Encoder** | 60M (decoder) | 89M (encoder) | 30 | 4 | Fast experiments, limited resources |\n",
    "| **2. Partial Unfreezing** | 80M (decoder + last 2 encoder layers) | 69M (first encoder layers) | 25 | 2 | Balanced performance/efficiency |\n",
    "| **3. Full Fine-Tuning** | 149M (all layers) | 0 | 20 | 2 | Maximum performance |\n",
    "\n",
    "**Shared Training Configuration**:\n",
    "\n",
    "All strategies use the same optimized hyperparameters:\n",
    "- **Gradient accumulation**: 8 steps for effective batch sizes of 32/16/16\n",
    "- **Learning rate**: `5e-5` with cosine annealing and 500-step warmup\n",
    "- **Optimizer**: AdamW, since it's adaptive and works well with document understanding models\n",
    "- **Precision**: Mixed FP16 because it's 2x faster and requires 50% less memory\n",
    "- **Gradient clipping**: 1.0 to prevent instability\n",
    "- **Validation**: 2Ã— per epoch for early overfitting detection\n",
    "- **Early stopping patience**: 10 epochs \n",
    "\n",
    "The strategies have different epochs and batch sizes because:\n",
    "- **Strategy 1 (30 epochs, batch 4)**: Only train the decoder is fast and can afford more iterations with larger batches\n",
    "- **Strategy 2 (25 epochs, batch 2)**: Moderate training time for balanced approach, reduced batch for memory\n",
    "- **Strategy 3 (20 epochs, batch 2)**: Fewer epochs to prevent overfitting on small dataset, smaller batch due to full model updates\n",
    "\n",
    "Each strategy has detailed explanation and training configuration in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3232f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Donut DataModule\n",
    "donut_dm = DonutDataModule(\n",
    "    hf_dataset=ds,\n",
    "    processor=donut_processor,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    use_augmentation=True,\n",
    ")\n",
    "\n",
    "donut_dm.setup()\n",
    "\n",
    "# Strategy 1: Freeze encoder completely (only train decoder)\n",
    "print(\"Strategy 1: Freeze Encoder - Train Decoder Only\")\n",
    "donut_model_frozen = DonutLightningModel(\n",
    "    model_name=\"naver-clova-ix/donut-base\",\n",
    "    learning_rate=3e-5,\n",
    "    freeze_encoder=True,\n",
    "    max_length=384,  # Reduced for 8GB RAM\n",
    "    unfreeze_last_n_layers=0,\n",
    ")\n",
    "\n",
    "# Strategy 2: Freeze encoder but unfreeze last 2 layers\n",
    "print(\"Strategy 2: Freeze Encoder - Unfreeze Last 2 Layers\")\n",
    "donut_model_partial = DonutLightningModel(\n",
    "    model_name=\"naver-clova-ix/donut-base\",\n",
    "    learning_rate=2e-5,\n",
    "    freeze_encoder=True,\n",
    "    max_length=384,  # Reduced for 8GB RAM\n",
    "    unfreeze_last_n_layers=2,\n",
    ")\n",
    "\n",
    "# Strategy 3: Full fine-tuning (unfreeze everything)\n",
    "print(\"Strategy 3: Full Fine-Tuning (All Layers Trainable)\")\n",
    "donut_model_full = DonutLightningModel(\n",
    "    model_name=\"naver-clova-ix/donut-base\",\n",
    "    learning_rate=1e-5,\n",
    "    freeze_encoder=False,\n",
    "    max_length=384,  # Reduced for 8GB RAM\n",
    "    unfreeze_last_n_layers=0,\n",
    ")\n",
    "\n",
    "print(\"All Donut models initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e90c5dd",
   "metadata": {},
   "source": [
    "## Donut Training - Strategy 1: Frozen Encoder\n",
    "\n",
    "In this first strategy, we will freeze encoder (Swin Transformer) and train only decoder (mBART). The configuration we'll implement is:\n",
    "- Epochs 30: more iterations since it should be the fastest\n",
    "- Batch size 4: to use a machine with approximately 20GB VRAM (A4500)\n",
    "- Gradient accumulation 8: effective batch = 32 (stable gradients for JSON generation)\n",
    "- Learning rate 5e-5: with cosine annealing and 500-step warmup\n",
    "- Precision FP16 mixed: it's 2 times faster and requires 50% less memory\n",
    "- Gradient clipping 1.0: prevents exploding gradients (important for sequence generation)\n",
    "- Validation 2x per epoch: early overfitting detection\n",
    "\n",
    "**Callbacks & Logging**:\n",
    "- ModelCheckpoint: \n",
    "    - Saves top 3 models by `val_loss` and last checkpoint\n",
    "    - Format: `donut-frozen-{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}.ckpt`\n",
    "    - Directory: `./donut_checkpoints/strategy1_frozen/`\n",
    "    - Enables resuming interrupted training\n",
    "- EarlyStopping:\n",
    "    - Patience: 10 epochs. Stops if `val_loss` doesn't improve (longer patience for structured generation)\n",
    "    - Prevents overfitting and saves compute time\n",
    "- CSVLogger:\n",
    "    - Saves all metrics (epoch, train_loss, val_loss, train_acc, val_acc)\n",
    "    - Directory: `./donut_logs/strategy1_frozen/metrics.csv`\n",
    "\n",
    "Additionally, we use gradient accumulation because the batch size of 32 won't fit even with 20GB VRAM, so the solution is to process 4 samples Ã— 8 times, and then updating the weights. With this, we expect same gradient quality as if the batch size is 32, but using the memory of a batch size of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a0428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure callbacks for Donut Strategy 1\n",
    "donut_checkpoint_s1 = ModelCheckpoint(\n",
    "    dirpath='./donut_checkpoints/strategy1_frozen',\n",
    "    filename='donut-frozen-{epoch:02d}-{val_loss:.4f}',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=3,\n",
    "    save_last=True,  # Save last checkpoint for resuming\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "donut_early_stop_s1 = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "donut_csv_logger_s1 = CSVLogger(save_dir='./donut_logs', name='strategy1_frozen')\n",
    "\n",
    "# Initialize trainer for Donut Strategy 1\n",
    "donut_trainer_s1 = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[donut_checkpoint_s1, donut_early_stop_s1],\n",
    "    logger=donut_csv_logger_s1,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    log_every_n_steps=10,\n",
    "    gradient_clip_val=1.0,  # Gradient clipping for stability\n",
    "    # precision='16-mixed',\n",
    ")\n",
    "\n",
    "print(\"Donut Strategy 1 Trainer configured\")\n",
    "print(f\"Checkpoints: {donut_checkpoint_s1.dirpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './donut_checkpoints/strategy1_frozen/last.ckpt'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Resuming Donut S1 from {checkpoint_path}\")\n",
    "    donut_trainer_s1.fit(donut_model_frozen, donut_dm, ckpt_path=checkpoint_path)\n",
    "else:\n",
    "    print(\"Starting Donut S1 from scratch\")\n",
    "    donut_trainer_s1.fit(donut_model_frozen, donut_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad6ee40",
   "metadata": {},
   "source": [
    "## Donut Evaluation - Strategy 1\n",
    "\n",
    "After training, we need to evaluate the final performance on the test set, which consists of 100 receipts. For this, we'll load the best checkpoint, with the lowest `val_loss` and run on the test set to compute the metrics.\n",
    "\n",
    "**Metrics**:\n",
    "- **Validation Loss (Cross-Entropy)**: measures model confidence in JSON token generation\n",
    "    - **< 0.3**: Excellent (very confident, valid JSON)\n",
    "    - **0.3-0.6**: Good\n",
    "    - **0.6-1.0**: Fair\n",
    "    - **> 1.0**: Poor (likely generates invalid JSON)\n",
    "- **Token-Level Accuracy**: percentage of correctly predicted JSON tokens:\n",
    "    - Example:\n",
    "        ```\n",
    "        Ground truth: [\"{\", \"\\\"\", \"text\", \"\\\"\", \":\", \" \", \"\\\"\", \"TOTAL\"] (8 tokens)\n",
    "        Prediction:   [\"{\", \"\\\"\", \"text\", \"\\\"\", \":\", \" \", \"\\\"\", \"TOTAL\"] (8 correct)\n",
    "        Accuracy: 8/8 = 100%\n",
    "        ```\n",
    "    - **â‰¥ 0.95**: Excellent (nearly perfect JSON structure)\n",
    "    - **0.90-0.95**: Good\n",
    "    - **0.85-0.90**: Fair\n",
    "    - **< 0.85**: Poor (malformed JSON likely)\n",
    "\n",
    "The expected results of this strategy, based on CORD-v2 benchmarks, are:\n",
    "- **Val loss**: 0.2-0.5\n",
    "- **Token accuracy**: 90-94%\n",
    "- **Training time**: 6-8 hours (with early stopping)\n",
    "- **JSON validity**: High (most outputs should be valid JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = donut_checkpoint_s1.best_model_path\n",
    "print(f\"Evaluating Donut S1: {best_model_path}\")\n",
    "donut_trainer_s1.test(donut_model_frozen, donut_dm, ckpt_path=best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbaf132",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint_d1 = donut_checkpoint_s1.best_model_path\n",
    "donut_model_s1_loaded = DonutLightningModel.load_from_checkpoint(best_checkpoint_d1)\n",
    "donut_model_s1_loaded.eval()\n",
    "\n",
    "test_dataloader = donut_dm.test_dataloader()\n",
    "test_batch = next(iter(test_dataloader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    pixel_values = test_batch['pixel_values']\n",
    "    generated_ids = donut_model_s1_loaded.model.generate(\n",
    "        pixel_values, \n",
    "        max_length=768,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=donut_model_s1_loaded.processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=donut_model_s1_loaded.processor.tokenizer.eos_token_id,\n",
    "    )\n",
    "    predictions = donut_model_s1_loaded.processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    target_sequences = test_batch['target_sequence']\n",
    "\n",
    "num_samples = min(3, len(predictions))\n",
    "for i in range(num_samples):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Prediction: {predictions[i]}\")\n",
    "    print(f\"Ground Truth: {target_sequences[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f06e0",
   "metadata": {},
   "source": [
    "## Donut Training - Strategy 2: Partial Unfreezing\n",
    "\n",
    "Now, we'll implement the second strategy for Donut model. Here, we'll unfreeze the last 2 Swin Transformer layers, while keeping the rest frozen. This allows the model to adapt higher-level visual features to receipt-specific document patterns, such as table structures, receipt headers, itemized lists, and price alignments, while preserving the low-level feature extraction learned from pre-training.\n",
    "\n",
    "**Changes compared to Strategy 1**:\n",
    "- **Trainable parameters**: 80M (decoder + last 2 Swin layers)\n",
    "- **Frozen parameters**: 69M (first Swin layers)\n",
    "- **Epochs**: 25, 5 fewer epochs since it should take longer per epoch\n",
    "- **Batch size**: 2, reduced due to memory constraints with more trainable params\n",
    "- **Training time**: approximately 7-9 hours because it's slower than Strategy 1 due to more layers to train\n",
    "\n",
    "We chose this approach because:\n",
    "- The lower Swin Transformer layers learn generic visual features like edges, patches, and basic textures, that's why we keep them frozen.\n",
    "- The higher Swin layers learn document-specific hierarchical patterns, that's why we unfreeze to adapt the model for receipt structure understanding.\n",
    "- Balances domain adaptation with training efficiency.\n",
    "\n",
    "The rest of the configuration remains the same, in terms of gradient accumulation (8 steps for effective batch 16), learning rate (5e-5), precision (FP16), callbacks (ModelCheckpoint, EarlyStopping, CSVLogger).\n",
    "\n",
    "The expected results of this strategy, based on CORD-v2 benchmarks, are:\n",
    "- **Val loss**: 0.15-0.4, which is lower than Strategy 1\n",
    "- **Token accuracy**: 92-96%, which is better than Strategy 1\n",
    "- **Training time**: 7-9 hours with early stopping\n",
    "- **JSON quality**: Better structure understanding, fewer field extraction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28441bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure callbacks for Donut Strategy 2\n",
    "donut_checkpoint_s2 = ModelCheckpoint(\n",
    "    dirpath='./donut_checkpoints/strategy2_partial',\n",
    "    filename='donut-partial-{epoch:02d}-{val_loss:.4f}',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=3,\n",
    "    save_last=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "donut_early_stop_s2 = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "donut_csv_logger_s2 = CSVLogger(save_dir='./donut_logs', name='strategy2_partial')\n",
    "\n",
    "# Initialize trainer for Donut Strategy 2\n",
    "donut_trainer_s2 = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[donut_checkpoint_s2, donut_early_stop_s2],\n",
    "    logger=donut_csv_logger_s2,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1 if torch.cuda.is_available() else 'auto',\n",
    "    log_every_n_steps=10,\n",
    "    gradient_clip_val=1.0,\n",
    ")\n",
    "\n",
    "print(\"Donut Strategy 2 Trainer configured\")\n",
    "print(f\"Checkpoints: {donut_checkpoint_s2.dirpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be454ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './donut_checkpoints/strategy2_partial/last.ckpt'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Resuming Donut S2 from {checkpoint_path}\")\n",
    "    donut_trainer_s2.fit(donut_model_partial, donut_dm, ckpt_path=checkpoint_path)\n",
    "else:\n",
    "    print(\"Starting Donut S2 from scratch\")\n",
    "    donut_trainer_s2.fit(donut_model_partial, donut_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f36841e",
   "metadata": {},
   "source": [
    "## Donut Evaluation - Strategy 2\n",
    "\n",
    "Same as before, after training Strategy 2, we load the best checkpoint and compute metrics on the 100-receipt test set to compare the performance against the first strategy. We expect better accuracy, since the model is adapting to visual features of receipt-specific document structure.\n",
    "\n",
    "- We expect a lower validation loss, due to better calibrated predictions (around 0.15-0.4)\n",
    "- Improved JSON structure quality with fewer missing or incorrect fields.\n",
    "- Higher token accuracy, around 92-96%.\n",
    "- Better handling of receipt-specific layout patterns like itemized lists, tables, and field alignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1315d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = donut_checkpoint_s2.best_model_path\n",
    "print(f\"Evaluating Donut S2: {best_model_path}\")\n",
    "donut_trainer_s2.test(donut_model_partial, donut_dm, ckpt_path=best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba839a3",
   "metadata": {},
   "source": [
    "## Donut Training - Strategy 3: Full Fine-tuning\n",
    "\n",
    "For the last strategy we'll implement for Donut, we are going to unfreeze all layers, encoder and decoder. This will provide maximum adaptation to the receipt domain, but requires careful training to avoid overfitting on our small dataset of 800 receipts. The following is what changed, compared to the other models:\n",
    "\n",
    "- **Trainable parameters**: 149M, which is equivalent to all layers\n",
    "- **Frozen parameters**: 0\n",
    "- **Epochs**: 20, fewer epochs to prevent overfitting.\n",
    "- **Batch size**: 2, same as Strategy 2 due to memory constraints\n",
    "- **Training time**: slowest since all parameters update.\n",
    "\n",
    "As for the batch size, gradient accumulation (8 steps for effective batch 16), learning rate (5e-5), callbacks, and other configuration parameters, they all remain the same as the previous strategies. We expect the following results:\n",
    "\n",
    "- **Val loss**: 0.1-0.35, which would be the best of all strategies if there's no overfitting.\n",
    "- **Token accuracy**: 94-97%, which is the highest accuracy\n",
    "- **Training time**: 8-11 hours with early stopping\n",
    "- **JSON quality**: Best field extraction and structure understanding if overfitting is avoided\n",
    "\n",
    "The trade-offs of using this strategy are:\n",
    "- Potentially the best performance, since the model can fully adapt to receipt document understanding\n",
    "- It learns receipt-specific characteristics at both low-level (edges, patches) and high-level (document structure) visual patterns\n",
    "- It is slower in training because all layers compute gradients and update weights\n",
    "- Has some risk of overfitting, due to the small dataset and the large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c1518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure callbacks for Donut Strategy 3\n",
    "donut_checkpoint_s3 = ModelCheckpoint(\n",
    "    dirpath='./donut_checkpoints/strategy3_full',\n",
    "    filename='donut-full-{epoch:02d}-{val_loss:.4f}',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=3,\n",
    "    save_last=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "donut_early_stop_s3 = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "donut_csv_logger_s3 = CSVLogger(save_dir='./donut_logs', name='strategy3_full')\n",
    "\n",
    "# Initialize trainer for Donut Strategy 3\n",
    "donut_trainer_s3 = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[donut_checkpoint_s3, donut_early_stop_s3],\n",
    "    logger=donut_csv_logger_s3,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1 if torch.cuda.is_available() else 'auto',\n",
    "    log_every_n_steps=10,\n",
    "    gradient_clip_val=1.0,\n",
    ")\n",
    "\n",
    "print(\"Donut Strategy 3 Trainer configured\")\n",
    "print(f\"Checkpoints: {donut_checkpoint_s3.dirpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9461bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './donut_checkpoints/strategy3_full/last.ckpt'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Resuming Donut S3 from {checkpoint_path}\")\n",
    "    donut_trainer_s3.fit(donut_model_full, donut_dm, ckpt_path=checkpoint_path)\n",
    "else:\n",
    "    print(\"Starting Donut S3 from scratch\")\n",
    "    donut_trainer_s3.fit(donut_model_full, donut_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88336a9",
   "metadata": {},
   "source": [
    "## Donut Evaluation - Strategy 3\n",
    "\n",
    "The evaluation process is the same. First, we load the best checkpoint and compute metrics on the 100-receipt test set, and then the results will guide our final recommendation when we compare against the best TrOCR model.\n",
    "\n",
    "Same as before, after training Strategy 3, we'll evaluate it on the test set to determine if full fine-tuning provides better performance or if the model overfits on the 800 receipts it got during training.\n",
    "\n",
    "- In the best case, we expect a val loss of approximately 0.1-0.35, token accuracy of 94%-97%.\n",
    "- We'll also examine JSON validity rate and field extraction accuracy.\n",
    "- If it overfits, the performance should be similar or worse than Strategy 2, since the model memorized training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = donut_checkpoint_s3.best_model_path\n",
    "print(f\"Evaluating Donut S3: {best_model_path}\")\n",
    "donut_trainer_s3.test(donut_model_full, donut_dm, ckpt_path=best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c851a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logs_d1 = pd.read_csv('./donut_logs/strategy1_frozen/version_0/metrics.csv')\n",
    "    logs_d2 = pd.read_csv('./donut_logs/strategy2_partial/version_0/metrics.csv')\n",
    "    logs_d3 = pd.read_csv('./donut_logs/strategy3_full/version_0/metrics.csv')\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    \n",
    "    ax.plot(logs_d1['epoch'], logs_d1['val_loss'], label='Strategy 1: Frozen', marker='o', linewidth=2)\n",
    "    ax.plot(logs_d2['epoch'], logs_d2['val_loss'], label='Strategy 2: Partial', marker='s', linewidth=2)\n",
    "    ax.plot(logs_d3['epoch'], logs_d3['val_loss'], label='Strategy 3: Full', marker='^', linewidth=2)\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Validation Loss', fontsize=12)\n",
    "    ax.set_title('Donut: Validation Loss Comparison Across Strategies', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Donut Strategy Comparison - Final Metrics\")\n",
    "    print(f\"Strategy 1 (Frozen) - Best Val Loss: {logs_d1['val_loss'].min():.4f}\")\n",
    "    print(f\"Strategy 2 (Partial)    - Best Val Loss: {logs_d2['val_loss'].min():.4f}\")\n",
    "    print(f\"Strategy 3 (Full)   - Best Val Loss: {logs_d3['val_loss'].min():.4f}\")\n",
    "    \n",
    "    best_losses = {\n",
    "        'Strategy 1 (Frozen)': logs_d1['val_loss'].min(),\n",
    "        'Strategy 2 (Partial)': logs_d2['val_loss'].min(),\n",
    "        'Strategy 3 (Full)': logs_d3['val_loss'].min()\n",
    "    }\n",
    "    best_strategy = min(best_losses, key=best_losses.get)\n",
    "    print(f\"Best Strategy: {best_strategy} with Val Loss = {best_losses[best_strategy]:.4f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Training logs not found. Please train the models first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
