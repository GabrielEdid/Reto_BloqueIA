{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e98b76",
   "metadata": {},
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0d77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Normalize\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from typing import Callable, Optional, List, Dict\n",
    "\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "import evaluate\n",
    "\n",
    "# Configure device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25195e5",
   "metadata": {},
   "source": [
    "## Load CORD-v2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e8047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CORD-v2 dataset\n",
    "print(\"Loading CORD-v2 dataset from Hugging Face:\")\n",
    "ds = load_dataset(\"naver-clova-ix/cord-v2\")\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"Dataset structure:\")\n",
    "print(ds)\n",
    "\n",
    "# Show dataset splits\n",
    "print(\"Available splits:\")\n",
    "for split in ds.keys():\n",
    "    print(f\"    - {split}: {len(ds[split])} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389800d",
   "metadata": {},
   "source": [
    "## Custom Transform Classes\n",
    "\n",
    "Define preprocessing transforms used in the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1ea199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLAHETransform:\n",
    "    \"\"\"Apply CLAHE to improve local contrast in images.\"\"\"\n",
    "    \n",
    "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clip_limit = clip_limit\n",
    "        self.tile_grid_size = tile_grid_size\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Convert PIL Image to numpy array\n",
    "        img_np = np.array(img)\n",
    "        \n",
    "        # Convert to LAB color space\n",
    "        lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n",
    "        \n",
    "        # Apply CLAHE to L channel\n",
    "        clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n",
    "        lab[:, :, 0] = clahe.apply(lab[:, :, 0])\n",
    "        \n",
    "        # Convert back to RGB\n",
    "        img_clahe = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "        \n",
    "        # Return as PIL Image\n",
    "        return Image.fromarray(img_clahe)\n",
    "\n",
    "\n",
    "class SharpenTransform:\n",
    "    \"\"\"Sharpen image to enhance text clarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, kernel_size=(5, 5), sigma=1.0, amount=1.5):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.sigma = sigma\n",
    "        self.amount = amount\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Convert PIL Image to numpy array\n",
    "        img_np = np.array(img)\n",
    "        \n",
    "        # Create blurred version\n",
    "        blurred = cv2.GaussianBlur(img_np, self.kernel_size, self.sigma)\n",
    "        \n",
    "        # Unsharp mask: original + amount * (original - blurred)\n",
    "        sharpened = cv2.addWeighted(img_np, 1.0 + self.amount, blurred, -self.amount, 0)\n",
    "        \n",
    "        # Clip values to valid range\n",
    "        sharpened = np.clip(sharpened, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Return as PIL Image\n",
    "        return Image.fromarray(sharpened)\n",
    "\n",
    "\n",
    "# Initialize transform instances\n",
    "clahe_transform = CLAHETransform(clip_limit=2.0, tile_grid_size=(8, 8))\n",
    "sharpen_transform = SharpenTransform(amount=1.0)\n",
    "\n",
    "print(\"Custom transforms defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d665cf00",
   "metadata": {},
   "source": [
    "# TrOCR: Transformer-based OCR for Receipt Text Extraction\n",
    "---\n",
    "\n",
    "TrOCR (Transformer-based Optical Character Recognition) is a model developed by Microsoft that combines computer vision and natural language processing to extract text from images. It differs from traditional OCR models since those rely on separate components, like detection, recognition and post-processing, but TrOCR uses a unified Transformer architecture to directly convert image pixels into text sequences. Reading text from receipts can be very challenging due to various factors such as variable quality, like photos taken in different lightning condiitons, angles, etc; complex layouts with multiple columns, tables and different font sizes; a lot of noise like shadows, creases, faded text and backgournd patterns; and business specific vocabulary like the sotre names, product codes, symbols, etc. Using TrOCR is very helpful for this because:\n",
    "- Pre-trained on large-scale data, which is why it learns different visual representations from millions of document images.\n",
    "- Focuses on relevant text regions, while ignoring noise.\n",
    "- Contains a sequence modeling that understands context and can correct errors based on linguistic patterns.\n",
    "- Transfer learning and leverages knowledge from ImageNet and text corpora.\n",
    "\n",
    "TrOCR follows an encoder-decoder architecture where:\n",
    "\n",
    "1. **Encoder: Visition Transform (ViT)**:\n",
    "    - **Input**: Receipt image in RGB and resized to fixed dimensions\n",
    "    - **Process**: Image is split into patches, each of with is treated as a token\n",
    "    - **Output**: Sequence of visual embeddings representing image content\n",
    "    - **Pre-trained on**: ImageNet-21K\n",
    "    - **What it does**: Extracts visual features and understands spatial relationships in the image\n",
    "2. **Decoder: RoBERTa (Transformer Language Model)**:\n",
    "    - **Input**: Visual embeddings from encoder\n",
    "    - **Process**: Generates text tokens autoregressively (one character/word at a time)\n",
    "    - **Output**: Text sequence representing all text in the receipt\n",
    "    - **Pre-trained on**: Large text corpora, like books, articles and web text.\n",
    "    - **What it does**: Converts visual features into coherent text using language understanding.\n",
    "    - It uses RoBERTa since it has strong language modeling capabilities and can correct OCR errors by leveraging linguistic context. An example of this can be predicting missing letters based on word structure.\n",
    "3. **Cross-Attention Mechanism**: the decoder attends to encoder outputs at each decoding step, allowing it to\n",
    "    - Focus on specific image regions when generating each character/word\n",
    "    - Align visual features with text tokens\n",
    "    - Handle variable-length inputs and outputs\n",
    "\n",
    "In the following steps, we are going to implement three fine-tuning strategies to adapt the pre-trained TrOCR model to receipts:\n",
    "1. **Strategy 1: Frozen Encoder (Feature Extraction)**:\n",
    "    - **Freeze**: All encoder layers (ViT)\n",
    "    - **Train**: Only decoder layers (RoBERTa)\n",
    "    - **Reason**: The encoder already knows how to extract visual features from documents, since it is pre-trained on printed text. We only need to adapt the decoder to receipt-specific vocabulary and layout.\n",
    "    - **Advantages**: Fast training, low memory usage, less chance of overfitting\n",
    "    - **Disadvantages**: Limited adaptation to receipt-specific visual patterns, such as shadows and creases.\n",
    "    - **Best for**: Small datasets, limited compute resources (Our dataset has 1000 receipts).\n",
    "2. **Strategy 2: Strategy 2: Partial Unfreezing (Progressive Fine-Tuning)**:\n",
    "    - **Freeze**: First N-3 encoder layers\n",
    "    - **Train**: Last 3 encoder layers + all decoder layers\n",
    "    - **Reason**: Lower layers learn generic features, like edges and textures, while higher layers learn task-specific patterns. By unfreezing the last layers, we allow the model to adapt to receipt-specific visual characteristics.\n",
    "    - **Advantages**: Better domain adaptation than frozen encoder\n",
    "    - **Disadvantages**: Requires more memory and compute than Strategy 1\n",
    "    - **Best for**: Medium-sized datasets with some computational budget\n",
    "3. **Strategy 3: Strategy 3: Full Fine-Tuning (End-to-End Training)**:\n",
    "    - **Freeze**: Nothing\n",
    "    - **Train**: All encoder + decoder layers\n",
    "    - **Reason**: Maximum adaptation to receipt domain. The model can learn receipt-specific visual features and text patterns simultaneously.\n",
    "    - **Advantages**: Best performance potential, due to full customization\n",
    "    - **Disadvantages**: Requires large dataset, high memory/compute, but has risk of overfitting\n",
    "    - **Best for**: Large datasets, sufficient compute, when domain shift is significant\n",
    "\n",
    "From the TrOCR available models, we are going to use `microsoft/trocr-base-printed` because:\n",
    "- Pre-trained specifically on printed text, like receipts.\n",
    "- Base size of 334M parameters, which balances performance and efficiency.\n",
    "- Strong performance on document OCR benchmarks, like CORD.\n",
    "\n",
    "An alternative is to use `microsoft/trocr-large-printed`, which is a larger model with aproximately 558M parameters, and it can produce a better accuracy, but at the cost of compute.\n",
    "\n",
    "But fine-tuning TrOCR o CORD-v2, we expect:\n",
    "- High accuracy on clean receipts\n",
    "- Robustness to common receipt variations, such as lightning, rotation, blur, etc.\n",
    "- Fast inference\n",
    "- Compare which fine-tuning strategy works best for our dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113f39c5",
   "metadata": {},
   "source": [
    "## TrOCR Dataset Preparation\n",
    "\n",
    "As we mentioned before, TrOCR is trained to perform to extract text from images.\n",
    "- **Input**: Receipt image (PIL Image)\n",
    "- **Output**: Plain text sequence (all text in the receipt, reading order)\n",
    "\n",
    "**CORD-v2 Dataset Structure**:\n",
    "\n",
    "The CORD-v2 dataset provides:\n",
    "- **Images**: Receipt images in PNG format\n",
    "- **Ground truth**: JSON annotations with:\n",
    "  - `valid_line`: List of text lines in the receipt\n",
    "  - `words`: Individual words with bounding boxes and text content\n",
    "  - `category`: Semantic labels (store name, date, total, etc.)\n",
    "\n",
    "**Data Extraction Process**:\n",
    "\n",
    "We need to convert the structured JSON annotations into plain text sequences. To achieve this, we will follow these steps:\n",
    "1. **Parse JSON**: Load the `ground_truth` string and parse it as JSON\n",
    "2. **Extract words**: Iterate through `valid_line`, then `words`, and then `text`\n",
    "3. **Concatenate**: Join all words with spaces to form a single text string\n",
    "4. **Example**:\n",
    "   - JSON: `{\"valid_line\": [{\"words\": [{\"text\": \"STORE\"}, {\"text\": \"NAME\"}]}]}`\n",
    "   - Output text: `\"STORE NAME\"`\n",
    "\n",
    "**Preprocessing Pipeline**:\n",
    "\n",
    "For each image, we will:\n",
    "1. Apply our custom transform, which are CLAHE and Sharpening to improve text clarity.\n",
    "2. Use TrOCR processor to handle resizing, normalization, and conversion to tensors internally.\n",
    "3. Apply tokenization to convert text into token IDs using RoBERTa tokenizer.\n",
    "4. Put in padding or truncation to sequences to a `max_length` of 512 tokens.\n",
    "\n",
    "Some special cases include:\n",
    "- **Padding tokens**: Replaced with `-100` in labels so they're ignored during loss computation\n",
    "- **Max length**: Set to 512 tokens to accommodate long receipts while fitting in GPU memory\n",
    "- **Error handling**: If JSON parsing fails, return empty string to prevents training crashes.\n",
    "\n",
    "The following class is created to combine CORD's structured annotations and TrOCR's expected input and output formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df91215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrOCRDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for TrOCR fine-tuning on CORD-v2.\n",
    "    Extracts full text from receipt images.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hf_dataset,\n",
    "        processor,\n",
    "        image_transform: Optional[Callable] = None,\n",
    "        max_length: int = 512,\n",
    "    ):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.processor = processor\n",
    "        self.image_transform = image_transform\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        print(f\"TrOCRDataset initialized with {len(self.hf_dataset)} samples\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def extract_text_from_ground_truth(self, ground_truth_str: str) -> str:\n",
    "        \"\"\"Extract all text from CORD ground truth JSON.\"\"\"\n",
    "        try:\n",
    "            gt_dict = json.loads(ground_truth_str)\n",
    "            \n",
    "            # Extract all text from 'valid_line' entries\n",
    "            text_lines = []\n",
    "            if 'valid_line' in gt_dict:\n",
    "                for line in gt_dict['valid_line']:\n",
    "                    if 'words' in line:\n",
    "                        for word in line['words']:\n",
    "                            if 'text' in word:\n",
    "                                text_lines.append(word['text'])\n",
    "            \n",
    "            # Join all text with spaces\n",
    "            full_text = ' '.join(text_lines)\n",
    "            return full_text.strip()\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        sample = self.hf_dataset[idx]\n",
    "        \n",
    "        # Get image\n",
    "        image = sample['image']\n",
    "        \n",
    "        # Apply custom preprocessing (CLAHE + Sharpening)\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        \n",
    "        # Extract text from ground truth\n",
    "        text = self.extract_text_from_ground_truth(sample['ground_truth'])\n",
    "        \n",
    "        # Process with TrOCR processor\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "        \n",
    "        # Tokenize text\n",
    "        labels = self.processor.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids.squeeze()\n",
    "        \n",
    "        # Replace padding token id with -100 (ignored in loss)\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels,\n",
    "            \"text\": text  # For evaluation\n",
    "        }\n",
    "\n",
    "# Test TrOCR dataset\n",
    "print(\"Loading TrOCR processor:\")\n",
    "trocr_processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "\n",
    "# TrOCR preprocessing: only CLAHE and Sharpening (no Normalize, no ToTensor)\n",
    "# The TrOCR processor handles conversion to tensor and normalization internally\n",
    "trocr_preprocess = transforms.Compose([\n",
    "    clahe_transform,\n",
    "    sharpen_transform,\n",
    "])\n",
    "\n",
    "# Create test dataset\n",
    "trocr_test_dataset = TrOCRDataset(\n",
    "    hf_dataset=ds['train'],\n",
    "    processor=trocr_processor,\n",
    "    image_transform=trocr_preprocess,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Test sample\n",
    "test_sample = trocr_test_dataset[0]\n",
    "print(f\"Sample output:\")\n",
    "print(f\"    - Pixel values shape: {test_sample['pixel_values'].shape}\")\n",
    "print(f\"    - Labels shape: {test_sample['labels'].shape}\")\n",
    "print(f\"    - Text preview: {test_sample['text'][:100]}...\")\n",
    "print(f\"TrOCR Dataset ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940dab3f",
   "metadata": {},
   "source": [
    "## TrOCR Model Implementation with PyTorch Lightning\n",
    "\n",
    "We will use PyTorch Lightning, since it's a high-level framework that simplifies training models:\n",
    "- **Organized code**: Separates research code from engineering code\n",
    "- **Built-in features**: Automatic logging, checkpointing, early stopping, multi-GPU support\n",
    "- **Reproducibility**: Handles random seeds, deterministic training\n",
    "- **Less boilerplate**: No need to manually write training loops, GPU transfer logic\n",
    "\n",
    "**Model Class Structure**: The following implementation of our `TrOCRLightningModel` inherits from `LightningModule` and implements:\n",
    "1. Initialization (`__init__`):\n",
    "    - Load pre-trained `VisionEncoderDecoderModel` from Hugging Face\n",
    "    - Load corresponding `TrOCRProcessor` to handle image preprocessing and tokenization.\n",
    "    - Configure generation parameters, like start token, padding, EOS token.\n",
    "    - Apply freezing strategy (freeze/unfreeze layers based on strategy)\n",
    "    - Initialize metrics, such as CER (Percentage of characters that are wrong), WER (Percentage of words that are wrong), and accuracy\n",
    "2. Freezing Strategy (`_apply_freezing_strategy`):\n",
    "    - Iterate through `model.encoder.parameters()` and set `requires_grad = False` to freeze\n",
    "    - For partial unfreezing: Access encoder layers via `model.encoder.encoder.layer[-N:]` and unfreeze\n",
    "    - Decoder is always trainable: `model.decoder.parameters()` have `requires_grad = True`\n",
    "    - We freeze parameters , since those don't compute gradients, which is equivalent to faster training and less memory. Also, pre-trained weigths and only task-specific layers adapt for better generalization on small datasets.\n",
    "3. Forward Pass (`forward`):\n",
    "    - Takes `pixel_values` (image tensors) and `labels` (text token IDs)\n",
    "    - Returns model outputs including loss and logits\n",
    "    - Loss is automatically computed by comparing logits with labels (cross-entropy)\n",
    "4. Training Step (`training_step`):\n",
    "    - This is what happens in each training iteration:\n",
    "        1. forward pass: `outputs = self(pixel_values, labels=labels)`\n",
    "        2. extract loss: `loss = outputs.loss`\n",
    "        3. generate predictions for accuracy: `model.generate()` creates text sequences\n",
    "        4. decode predictions and references to text strings.\n",
    "        5. Compute CER (Character Error Rate) and convert to accuracy (1 - CER)\n",
    "        6. Log metrics: `self.log('train_loss', loss)`\n",
    "        7. Return loss (Lightning automatically calls `loss.backward()` and optimizer step)\n",
    "    - Generating predictions during training is expensive, but very useful for monitoring.\n",
    "5. Validation Step (`validation_step`):\n",
    "    - Evaluate model on validation set to monitor overfitting\n",
    "        1. Forward pass, same as training.\n",
    "        2. Generate predictions: `model.generate(pixel_values, max_length=384)`\n",
    "        3. Decode predictions and ground truth\n",
    "        4. Compute metrics: CER, WER (Word Error Rate), accuracy\n",
    "        5. Log metrics: `self.log('val_loss', loss, 'val_cer', cer, ...)`\n",
    "    - CER vs WER: measures character-level errors, like insertions, deletions and usbtitutions.\n",
    "        - Formula: `(substitutions + deletions + insertions) / total_characters`\n",
    "        - Lower is better (0 = perfect)\n",
    "    - WER: same, but at word level\n",
    "        - More interpretable for human readers\n",
    "        - Stricter: One wrong character = entire word wrong\n",
    "6. Test Step (`test_step`):\n",
    "    - Same as validation step but runs on the test data\n",
    "    - Final evaluation after training is complete\n",
    "7. Optimizer Configuration (`configure_optimizers`)\n",
    "    - AdamW Optimizer:\n",
    "        - Variant of Adam with decoupled weight decay to prevent overfitting\n",
    "        - Learning rate: 5e-5, which is very used for fine-tuning Transformers\n",
    "        - Weight decay: 0.01, which is L2 regularization.\n",
    "    - ReduceLROnPlateau Scheduler:\n",
    "        - Reduces lerning rate when validation loss plateaus\n",
    "        - Factor: 0.5 (reduce LR by half)\n",
    "        - Patience: 3 epochs, which means that it waits 3 epochs before reducing.\n",
    "        - Helps model converge to better local minima\n",
    "\n",
    "Some key variables are:\n",
    "1. `max_length=256`: limit inference time and memory usage and at the same time it prevents infinite loops in generation. We decided on 256, since receipts usually have 100-200 tokens.\n",
    "2. We replace padding with `-100` in labels, since PyTroch's `CrossEntropyLoss` ignores index `-100`, which ensures padding tokens don't contribute to loss and prevents the model from learning to predict padding.\n",
    "3. We have log metrics with `prog_bar=True` to display metrics in real-time during training, which helps us monitor training progress.\n",
    "\n",
    "This implementation follows best practices for fine-tuning vision-language models and provides a clean, maintainable, codebase for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58694f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrOCRLightningModel(L.LightningModule):\n",
    "    \"\"\"\n",
    "    TrOCR model with PyTorch Lightning for OCR on receipts.\n",
    "    \n",
    "    Fine-tuning strategy:\n",
    "    - Phase 1: Freeze encoder, train decoder only\n",
    "    - Phase 2: Unfreeze last encoder layers\n",
    "    - Phase 3: Full fine-tuning\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"microsoft/trocr-base-printed\",\n",
    "        learning_rate: float = 5e-5,\n",
    "        freeze_encoder: bool = True,\n",
    "        unfreeze_last_n_layers: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Load pre-trained TrOCR model\n",
    "        self.model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "        self.processor = TrOCRProcessor.from_pretrained(model_name)\n",
    "        \n",
    "        # Configure model generation parameters\n",
    "        self.model.config.decoder_start_token_id = self.processor.tokenizer.cls_token_id\n",
    "        self.model.config.pad_token_id = self.processor.tokenizer.pad_token_id\n",
    "        self.model.config.eos_token_id = self.processor.tokenizer.sep_token_id\n",
    "        \n",
    "        # Apply freezing strategy\n",
    "        self._apply_freezing_strategy(freeze_encoder, unfreeze_last_n_layers)\n",
    "        \n",
    "        # Metrics\n",
    "        self.cer_metric = evaluate.load(\"cer\")  # Character Error Rate\n",
    "        self.wer_metric = evaluate.load(\"wer\")  # Word Error Rate\n",
    "        \n",
    "        # Accuracy metrics from torchmetrics (character-level accuracy approximation: 1 - CER)\n",
    "        self.train_acc = torchmetrics.MeanMetric() # Stores 1 - CER for training\n",
    "        self.val_acc = torchmetrics.MeanMetric() # Stores 1 - CER for validation\n",
    "        \n",
    "    def _apply_freezing_strategy(self, freeze_encoder: bool, unfreeze_last_n_layers: int):\n",
    "        \"\"\"\n",
    "        Apply layer freezing strategy\n",
    "        \n",
    "        Args:\n",
    "            freeze_encoder: If True, freeze encoder backbone\n",
    "            unfreeze_last_n_layers: Number of last encoder layers to unfreeze\n",
    "        \"\"\"\n",
    "        if freeze_encoder:\n",
    "            # Freeze all encoder parameters\n",
    "            for param in self.model.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Encoder frozen (transfer learning mode)\")\n",
    "            \n",
    "            # Unfreeze last N layers if specified\n",
    "            if unfreeze_last_n_layers > 0:\n",
    "                # Access ViT encoder layers\n",
    "                encoder_layers = self.model.encoder.encoder.layer\n",
    "                for layer in encoder_layers[-unfreeze_last_n_layers:]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = True\n",
    "                print(f\"Unfroze last {unfreeze_last_n_layers} encoder layers\")\n",
    "        else:\n",
    "            print(\"Encoder unfrozen (full fine-tuning mode)\")\n",
    "        \n",
    "        # Decoder is always trainable\n",
    "        for param in self.model.decoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Decoder trainable\")\n",
    "        \n",
    "        # Count trainable parameters\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"Trainable parameters: {trainable_params:,} / {total_params:,} \"\n",
    "              f\"({100 * trainable_params / total_params:.2f}%)\")\n",
    "    \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        return self.model(pixel_values=pixel_values, labels=labels)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        outputs = self(pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Generate predictions for accuracy calculation\n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(pixel_values, max_length=256)\n",
    "            generated_texts = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            labels_copy = labels.clone()\n",
    "            labels_copy[labels_copy == -100] = self.processor.tokenizer.pad_token_id\n",
    "            reference_texts = self.processor.batch_decode(labels_copy, skip_special_tokens=True)\n",
    "            \n",
    "            # Calculate accuracy (1 - CER)\n",
    "            cer = self.cer_metric.compute(predictions=generated_texts, references=reference_texts)\n",
    "            acc = max(0.0, 1.0 - cer)  # Convert CER to accuracy\n",
    "            self.train_acc.update(acc)\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        outputs = self(pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Generate predictions for metrics\n",
    "        generated_ids = self.model.generate(pixel_values, max_length=384)\n",
    "        generated_texts = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Decode ground truth\n",
    "        labels_copy = labels.clone()\n",
    "        labels_copy[labels_copy == -100] = self.processor.tokenizer.pad_token_id\n",
    "        reference_texts = self.processor.batch_decode(labels_copy, skip_special_tokens=True)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        cer = self.cer_metric.compute(predictions=generated_texts, references=reference_texts)\n",
    "        wer = self.wer_metric.compute(predictions=generated_texts, references=reference_texts)\n",
    "        acc = max(0.0, 1.0 - cer)  # Convert CER to accuracy\n",
    "        self.val_acc.update(acc)\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_cer', cer, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_wer', wer, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_acc', self.val_acc, prog_bar=True, on_epoch=True)\n",
    "        \n",
    "        return {'val_loss': loss, 'val_cer': cer, 'val_wer': wer, 'val_acc': acc}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        # Generate predictions\n",
    "        generated_ids = self.model.generate(pixel_values, max_length=384)\n",
    "        generated_texts = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Decode ground truth\n",
    "        labels_copy = labels.clone()\n",
    "        labels_copy[labels_copy == -100] = self.processor.tokenizer.pad_token_id\n",
    "        reference_texts = self.processor.batch_decode(labels_copy, skip_special_tokens=True)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        cer = self.cer_metric.compute(predictions=generated_texts, references=reference_texts)\n",
    "        wer = self.wer_metric.compute(predictions=generated_texts, references=reference_texts)\n",
    "        acc = max(0.0, 1.0 - cer)  # Convert CER to accuracy\n",
    "        \n",
    "        self.log('test_cer', cer, prog_bar=True)\n",
    "        self.log('test_wer', wer, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        \n",
    "        return {'test_cer': cer, 'test_wer': wer, 'test_acc': acc}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=3\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss'\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"TrOCR Lightning Model defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd4f0f",
   "metadata": {},
   "source": [
    "## TrOCR DataModule with Custom Preprocessing\n",
    "\n",
    "Now, we will implement a `LightningDataModule` to encapsulate all data-related logic in one place, such as:\n",
    "- Dataset creation (train/val/test splits)\n",
    "- Data transformations and augmentations\n",
    "- DataLoader configuration, like batch size, shuffling and workers.\n",
    "- Reproducibility to ensure consistent data handling across experiments.\n",
    "\n",
    "To not rely only in TrOCR processor, we use our custom preprocessing first:\n",
    "- **CLAHE (Contrast Limited Adaptive Histogram Equalization)**: enhances local contrast without amplifying noise.\n",
    "- **Sharpening**: improves text edge definition for better OCR accuracy.\n",
    "- Applying first our custom transform is very important because it improves the image quality of the receipts. Then, using TrOCR Processor helps us resize, normalize and convert to tensor with the purpose of preparing for model input.\n",
    "\n",
    "**Augmentation Strategy**: \n",
    "- Training step: we will apply augmentations to increase the dataset diversity.\n",
    "    - Random 5 degree rotation to simulate camera angles.\n",
    "    - Color jitter for brightness and contrast to simulate lightning variations.\n",
    "- Validation and Test sets: no augmentation, only preprocessing.\n",
    "    - We want to measure true performance on clean, processed images.\n",
    "    - Augmentation during validation would give misleading metrics.\n",
    "\n",
    "**DataLoader Configuration**:\n",
    "- **Batch size**: Set to 1, due to memory issues.\n",
    "- **Shuffle**: True for training for random order, but False for validation and test\n",
    "- **num_workers**: Set to 0 to avoid multiprocessing issues on some systems\n",
    "- **collate_fn**: Custom function to properly batch mixed data types (tensors and text strings)\n",
    "\n",
    "\n",
    "**Collate Function Explained**: The main issue is that PyTorch's default collate can't handle mixed types like tensors and list of strings. For this, we created a custom collate function that:\n",
    "1. Stacks `pixel_values` tensors into batch: `torch.stack([item['pixel_values'] for item in batch])`\n",
    "2. Stacks `labels` tensors into batch: `torch.stack([item['labels'] for item in batch])`\n",
    "3. Keeps text as list: `[item['text'] for item in batch]` (used for evaluation decoding)\n",
    "\n",
    "This will ensure us proper batching, while preserving text references for metrix computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b982f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrOCRDataModule(L.LightningDataModule):\n",
    "    \"\"\"DataModule for TrOCR training with CORD-v2 dataset.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hf_dataset,\n",
    "        processor,\n",
    "        batch_size: int = 1,\n",
    "        num_workers: int = 0,\n",
    "        use_augmentation: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.processor = processor\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.use_augmentation = use_augmentation\n",
    "        \n",
    "        # Define preprocessing transforms, reusing our custom transforms\n",
    "        self.train_transform = transforms.Compose([\n",
    "            CLAHETransform(clip_limit=2.0, tile_grid_size=(8, 8)),\n",
    "            SharpenTransform(amount=1.0),\n",
    "            transforms.RandomRotation(degrees=5, fill=255) if use_augmentation else transforms.Lambda(lambda x: x),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1) if use_augmentation else transforms.Lambda(lambda x: x),\n",
    "        ])\n",
    "        \n",
    "        self.val_transform = transforms.Compose([\n",
    "            CLAHETransform(clip_limit=2.0, tile_grid_size=(8, 8)),\n",
    "            SharpenTransform(amount=1.0),\n",
    "        ])\n",
    "    \n",
    "    def setup(self, stage: str = None):\n",
    "        self.train_dataset = TrOCRDataset(\n",
    "            hf_dataset=self.hf_dataset['train'],\n",
    "            processor=self.processor,\n",
    "            image_transform=self.train_transform,\n",
    "        )\n",
    "        \n",
    "        self.val_dataset = TrOCRDataset(\n",
    "            hf_dataset=self.hf_dataset['validation'],\n",
    "            processor=self.processor,\n",
    "            image_transform=self.val_transform,\n",
    "        )\n",
    "        \n",
    "        self.test_dataset = TrOCRDataset(\n",
    "            hf_dataset=self.hf_dataset['test'],\n",
    "            processor=self.processor,\n",
    "            image_transform=self.val_transform,\n",
    "        )\n",
    "        \n",
    "        print(f\"TrOCRDataModule Train: {len(self.train_dataset)}, \"\n",
    "              f\"Val: {len(self.val_dataset)}, Test: {len(self.test_dataset)}\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"Custom collate function to handle batching.\n",
    "        \n",
    "        This is necessary to properly batch mixed data types: tensors (pixel_values, labels) \n",
    "        are stacked, while text strings are kept as a list.\n",
    "        \"\"\"\n",
    "        pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "        labels = torch.stack([item['labels'] for item in batch])\n",
    "        texts = [item['text'] for item in batch]\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': labels,\n",
    "            'text': texts,\n",
    "        }\n",
    "\n",
    "print(\"TrOCR DataModule defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e950c2",
   "metadata": {},
   "source": [
    "## TrOCR Training Setup\n",
    "\n",
    "We train three independent TrOCR models using different fine-tuning strategies to compare performance vs computational cost.\n",
    "\n",
    "**Three Strategies**:\n",
    "\n",
    "| Strategy | Trainable Params | Frozen Params | Epochs | Use Case |\n",
    "|----------|-----------------|---------------|--------|----------|\n",
    "| **1. Frozen Encoder** | 89M (decoder) | 245M (encoder) | 30 | Fast experiments, limited resources |\n",
    "| **2. Partial Unfreezing** | 120M (decoder + last 3 encoder layers) | 214M (first encoder layers) | 25 | Balanced performance/efficiency |\n",
    "| **3. Full Fine-Tuning** | 334M (all layers) | 0 | 20 | Maximum performance |\n",
    "\n",
    "**Shared Training Configuration**:\n",
    "\n",
    "All strategies use the same optimized hyperparameters:\n",
    "- **Batch size**: 2\n",
    "- **Gradient accumulation**: 16 steps\n",
    "- **Learning rate**: `1e-4` with cosine annealing and 500-step warmup\n",
    "- **Optimizer**: AdamW, since it's adaptive and works well with single LR across strategies\n",
    "- **Precision**: Mixed FP16 because it's 2x faster and requires 50% less memory\n",
    "- **Gradient clipping**: 1.0 to prevent instability\n",
    "- **Validation**: 2× per epoch for early overfitting detection\n",
    "\n",
    "The strategies have differnt epochs because:\n",
    "- **Strategy 1 (30 epochs)**: Decoder-only training is fast and can afford more iterations\n",
    "- **Strategy 2 (25 epochs)**: Moderate training time for balanced approach\n",
    "- **Strategy 3 (20 epochs)**: Fewer epochs to prevent overfitting on small dataset, in this case of 800 receipts.\n",
    "\n",
    "Each strategy has detailed explanation and training configuration in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95586601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TrOCR DataModule\n",
    "trocr_dm = TrOCRDataModule(\n",
    "    hf_dataset=ds,\n",
    "    processor=trocr_processor,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    use_augmentation=True,\n",
    ")\n",
    "\n",
    "trocr_dm.setup()\n",
    "\n",
    "# Strategy 1: Freeze encoder completely (only train decoder)\n",
    "print(\"----- Strategy 1: Freeze Encoder - Train Decoder Only -----\")\n",
    "trocr_model_frozen = TrOCRLightningModel(\n",
    "    model_name=\"microsoft/trocr-base-printed\",\n",
    "    learning_rate=5e-5,\n",
    "    freeze_encoder=True,\n",
    "    unfreeze_last_n_layers=0,\n",
    ")\n",
    "\n",
    "# Strategy 2: Freeze encoder but unfreeze last 3 layers\n",
    "print(\"----- Strategy 2: Freeze Encoder - Unfreeze Last 3 Layers -----\")\n",
    "trocr_model_partial = TrOCRLightningModel(\n",
    "    model_name=\"microsoft/trocr-base-printed\",\n",
    "    learning_rate=3e-5,\n",
    "    freeze_encoder=True,\n",
    "    unfreeze_last_n_layers=3,\n",
    ")\n",
    "\n",
    "# Strategy 3: Full fine-tuning (unfreeze everything)\n",
    "print(\"----- Strategy 3: Full Fine-Tuning (All Layers Trainable) -----\")\n",
    "trocr_model_full = TrOCRLightningModel(\n",
    "    model_name=\"microsoft/trocr-base-printed\",\n",
    "    learning_rate=2e-5,\n",
    "    freeze_encoder=False,\n",
    "    unfreeze_last_n_layers=0,\n",
    ")\n",
    "\n",
    "print(\"All TrOCR models initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4ee69",
   "metadata": {},
   "source": [
    "## TrOCR Training - Strategy 1: Frozen Encoder\n",
    "\n",
    "In this first strategy, we will freece encoder (ViT) and train only decoder (RoBERTa). The configuration we'll implement is:\n",
    "- Epochs 30: more iterations since it should be the fastest\n",
    "- Batch size 2: to use a machine with aproximately 12GB VRAM\n",
    "- Gradient accumulation 16: effective batch = 32 (stable gradients)\n",
    "- Learning rate 1e-4: with cosine annealing and 500-step warmup\n",
    "- Precision FP16 mixed: it's 2 times faster and requires less 50% less memory\n",
    "- Gradient clipping 1.0: prevents exploding gradients\n",
    "- Validation 2x per epoch: early overfitting detection\n",
    "\n",
    "**Callbacks & Logging**:\n",
    "- ModelCheckpoint: \n",
    "    - Saves top 3 models by  `val_loss` and last checkpoint\n",
    "    - Format: `trocr-{epoch:02d}-{val_loss:.4f}-{val_acc:.4f}.ckpt`\n",
    "    - Directory: `./trocr_checkpoints/strategy_frozen/`\n",
    "    - Enables resuming interrupted training\n",
    "- EarlyStopping:\n",
    "    - Patience: 5 epochs. Stops if `val_loss` doesn't improve\n",
    "    - Prevents overfitting and saves compute time\n",
    "- CSVLogger:\n",
    "    - Saves all metrics (epoch, train_loss, val_loss, train_acc, val_acc)\n",
    "    - Directory: `./trocr_logs/strategy_frozen/metrics.csv`\n",
    "\n",
    "Additionally, we use gradient accumulation because the batch size of 32 won't fit a 12GB VRAM machine, so the solution is to process 2 samples x 16 times, and then updating the weights. With this, we expect same gradient quality as if the batch size is 32, but using the memory of a batch size of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "trocr_checkpoint_s1 = ModelCheckpoint(\n",
    "    dirpath='./trocr_checkpoints/strategy1_frozen',\n",
    "    filename='trocr-frozen-{epoch:02d}-{val_loss:.4f}',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=3,\n",
    "    save_last=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "trocr_early_stop_s1 = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "trocr_csv_logger_s1 = CSVLogger(save_dir='./trocr_logs', name='strategy1_frozen')\n",
    "\n",
    "trocr_trainer_s1 = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[trocr_checkpoint_s1, trocr_early_stop_s1],\n",
    "    logger=trocr_csv_logger_s1,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1 if torch.cuda.is_available() else 'auto',\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "print(\"TrOCR Strategy 1 Trainer configured\")\n",
    "print(f\"    - Checkpoints: {trocr_checkpoint_s1.dirpath}\")\n",
    "print(f\"    - Logs: trocr_logs/strategy1_frozen\")\n",
    "\n",
    "checkpoint_files = glob.glob('./trocr_checkpoints/strategy1_frozen/*.ckpt')\n",
    "if checkpoint_files:\n",
    "    print(f\"    - Found {len(checkpoint_files)} existing checkpoint(s)\")\n",
    "else:\n",
    "    print(f\"    - No existing checkpoints found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abeda61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training from scratch, or resume from last checkpoint if exists\n",
    "checkpoint_path = './trocr_checkpoints/strategy1_frozen/last.ckpt'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Resuming training from {checkpoint_path}\")\n",
    "    trocr_trainer_s1.fit(trocr_model_frozen, trocr_dm, ckpt_path=checkpoint_path)\n",
    "else:\n",
    "    print(\"Starting training from scratch\")\n",
    "    trocr_trainer_s1.fit(trocr_model_frozen, trocr_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fa2cba",
   "metadata": {},
   "source": [
    "## TrOCR Evaluation - Strategy 1\n",
    "\n",
    "After training, we need to evaluate the final performance on the test set, which consists of 100 receipts. For this, we'll load the best checkpoint, with the lowest `val_loss` and run on the test set to compute the metrics.\n",
    "\n",
    "**Metrics**:\n",
    "- **Validation Loss (Cross-Entropy)**: measures model confidence and accuracy\n",
    "    - **< 0.5**: Excellent (very confident)\n",
    "    - **0.5-1.0**: Good\n",
    "    - **1.0-2.0**: Fair\n",
    "    - **> 2.0**: Poor\n",
    "- **Token-Level Accuracy**: percentrage of correctly predicted tokens:\n",
    "    - Example:\n",
    "        ```\n",
    "        Ground truth: [\"TO\", \"TAL\", \":\", \" \", \"$\", \"45\", \".\", \"99\"]  (8 tokens)\n",
    "        Prediction:   [\"TO\", \"TAL\", \":\", \" \", \"$\", \"45\", \".\", \"9\"]   (7 correct)\n",
    "        Accuracy: 7/8 = 87.5%\n",
    "        ```\n",
    "    - - **≥ 0.95**: Excellent\n",
    "    - **0.90-0.95**: Good\n",
    "    - **0.85-0.90**: Fair\n",
    "    - **< 0.85**: Poor\n",
    "- **Optimal Manual Metrics**:\n",
    "    - C**haracter Error Rate (CER)**:\n",
    "        - Formula: `(Substitutions + Deletions + Insertions) / Total Characters`\n",
    "        - Example: `\"TOTAL: $45.99\"` → `\"TOTAL: $45.9\"` = 1/13 = 7.7% error\n",
    "    - **Word Error rate (WER)**:\n",
    "        - Same as CER but at word level\n",
    "        - More sensitive because one wrong character means the entire word is wrong.\n",
    "\n",
    "The expected results of this strategy, based on CORD-v2 benchmarks, are:\n",
    "- **Val loss**: 0.3-0.6\n",
    "- **Token accuracy**: 88-93%\n",
    "- **Training time**: 5-7 hours (with early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d8936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = trocr_checkpoint_s1.best_model_path\n",
    "print(f\"Evaluating best model: {best_model_path}\")\n",
    "trocr_trainer_s1.test(trocr_model_frozen, trocr_dm, ckpt_path=best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fcf38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint_s1 = trocr_checkpoint_s1.best_model_path\n",
    "trocr_model_s1_loaded = TrOCRLightningModel.load_from_checkpoint(best_checkpoint_s1)\n",
    "trocr_model_s1_loaded.eval()\n",
    "\n",
    "test_dataloader = trocr_dm.test_dataloader()\n",
    "test_batch = next(iter(test_dataloader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    pixel_values = test_batch['pixel_values']\n",
    "    generated_ids = trocr_model_s1_loaded.model.generate(pixel_values, max_length=512)\n",
    "    predictions = trocr_model_s1_loaded.processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    labels = test_batch['labels'].clone()\n",
    "    labels[labels == -100] = trocr_model_s1_loaded.processor.tokenizer.pad_token_id\n",
    "    ground_truths = trocr_model_s1_loaded.processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "num_samples = min(3, len(predictions))\n",
    "for i in range(num_samples):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Prediction: {predictions[i][:200]}...\")\n",
    "    print(f\"Ground Truth: {ground_truths[i][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a9d359",
   "metadata": {},
   "source": [
    "## TrOCR Training - Strategy 2: Partial Unfreezing\n",
    "\n",
    "Now, we'll implement the second strategy for TrOCR model. Here, we'll unfreeze the last 3 encoder layers, while keeping the rest frozen. This allows the model to adapt, in a higher level, to the visual features of receipt-specific patterns, such as printed numbers, tables, stamps, etc, while preserving the low-level feature extraction learned from the pre-training.\n",
    "\n",
    "**Changes compared to Strategy 1**:\n",
    "- **Trainable parameters**: 120M (decoder + last 3 encoder layers)\n",
    "- **Frozen parameters**: 214M (first encoder layers)\n",
    "- **Epochs**: 25, 5 lees epochs, since it should take longer\n",
    "- **Training time**: aproximately 6-8 hours because it's slower than Strategy 1 due to more layers to train\n",
    "\n",
    "We chose this aproach because:\n",
    "- The lower encoder layers can learn generic features like eges, textures and basic shapes, that's why we keep them frozen.\n",
    "- The higher encoder layers can learn receipt-specific patterns, that's why we unfreeze to adapt the model for receipts.\n",
    "- Balances domain adaptation with training efficiency.\n",
    "\n",
    "The rest of the configuration remains the same, in terms of the batch size, gradient accumulation, learning rate, precision, callbacks.\n",
    "\n",
    "The expected results of this strategy, based on CORD-v2 benchmarks, are:\n",
    "- **Val loss**: 0.2-0.5, which is lower than Strategy 1\n",
    "- **Token accuracy**: 91-95%, which is better than Strategy 1\n",
    "- **Training time**: 6-8 hours with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e95727",
   "metadata": {},
   "outputs": [],
   "source": [
    "trocr_checkpoint_s2 = ModelCheckpoint(\n",
    "    dirpath='./trocr_checkpoints/strategy2_partial',\n",
    "    filename='trocr-partial-{epoch:02d}-{val_loss:.4f}',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=3,\n",
    "    save_last=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "trocr_early_stop_s2 = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "trocr_csv_logger_s2 = CSVLogger(save_dir='./trocr_logs', name='strategy2_partial')\n",
    "\n",
    "trocr_trainer_s2 = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[trocr_checkpoint_s2, trocr_early_stop_s2],\n",
    "    logger=trocr_csv_logger_s2,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1 if torch.cuda.is_available() else 'auto',\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "print(\"TrOCR Strategy 2 Trainer configured\")\n",
    "print(f\"    - Checkpoints: {trocr_checkpoint_s2.dirpath}\")\n",
    "\n",
    "checkpoint_files = glob.glob('./trocr_checkpoints/strategy2_partial/*.ckpt')\n",
    "if checkpoint_files:\n",
    "    print(f\"    - Found {len(checkpoint_files)} existing checkpoint(s)\")\n",
    "else:\n",
    "    print(f\"    - No existing checkpoints found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f14bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './trocr_checkpoints/strategy2_partial/last.ckpt'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Resuming training from {checkpoint_path}\")\n",
    "    trocr_trainer_s2.fit(trocr_model_partial, trocr_dm, ckpt_path=checkpoint_path)\n",
    "else:\n",
    "    print(\"Starting training from scratch\")\n",
    "    trocr_trainer_s2.fit(trocr_model_partial, trocr_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc94a69",
   "metadata": {},
   "source": [
    "## TrOCR Evaluation - Strategy 2\n",
    "\n",
    "Same as before, after training Strategy 2, we evaluate on the test set to compare the performance against the first strategy. We expect better accuracy, since the model is adapting to visual features of receipt-specific characteristics.\n",
    "- We expect a lower validation loss, due to better calibrated predictions (around 0.2-0.5)\n",
    "- Higher token accuracy, around 91-95%.\n",
    "- Better handling of receipt-specific visual patterns like faded text, stamps, and table structures.\n",
    "\n",
    "The evaluation process is the same as before, we load the best checkpoint and compute metrics on the 100-receipt test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f76301",
   "metadata": {},
   "outputs": [],
   "source": [
    "trocr_trainer_s2.test(trocr_model_partial, trocr_dm, ckpt_path=trocr_checkpoint_s2.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a00de63",
   "metadata": {},
   "source": [
    "## TrOCR Training - Strategy 3: Full Fine-tuning\n",
    "\n",
    "For the last strategy we'll implement for TrOCR, we are going to unfreeze all layers, encoder and decoder. This will provide maximum adaptation to the receipt domain, but required careful training to avoid overfitting on our small dataset of 800 receipts. The following is what changed, compared to the other models:\n",
    "- **Trainable parameters**: 334M, which is equivalent to all layers\n",
    "- **Frozen parameters**: 0\n",
    "- **Epochs**: 20, fewer epochs to prevent overfitting.\n",
    "- **Training time**: slowest since all parameters update.\n",
    "\n",
    "The trade-offs of using this strategy are:\n",
    "- Potentially the best performance, since the model can fully adapt to receipt domain\n",
    "- It learns receipt-specific characteristics at both low-level and high-level visual patterns\n",
    "- Has some risk of overfitting, due to the small dataset and the large model\n",
    "- It is slower in training because all layers compute gradients and update weights\n",
    "\n",
    "As for the batch_size, gradient accumulation, learning rate, callbacks, and other configuration parameters, they all remain the same as the previous strategies. We expect the following results:\n",
    "- **Val loss**: 0.15-0.4, which would be the best of all strategies if there's no overfitting.\n",
    "- **Token accuracy**: 93-96%, which is the highest accuracy\n",
    "- **Training time**: 7-10 hours with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a9c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "trocr_checkpoint_s3 = ModelCheckpoint(\n",
    "    dirpath='./trocr_checkpoints/strategy3_full',\n",
    "    filename='trocr-full-{epoch:02d}-{val_loss:.4f}',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=3,\n",
    "    save_last=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "trocr_early_stop_s3 = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "trocr_csv_logger_s3 = CSVLogger(save_dir='./trocr_logs', name='strategy3_full')\n",
    "\n",
    "trocr_trainer_s3 = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[trocr_checkpoint_s3, trocr_early_stop_s3],\n",
    "    logger=trocr_csv_logger_s3,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1 if torch.cuda.is_available() else 'auto',\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "print(\"TrOCR Strategy 3 Trainer configured\")\n",
    "print(f\"    - Checkpoints: {trocr_checkpoint_s3.dirpath}\")\n",
    "\n",
    "checkpoint_files = glob.glob('./trocr_checkpoints/strategy3_full/*.ckpt')\n",
    "if checkpoint_files:\n",
    "    print(f\"    - Found {len(checkpoint_files)} existing checkpoint(s)\")\n",
    "else:\n",
    "    print(f\"    - No existing checkpoints found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3625c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './trocr_checkpoints/strategy3_full/last.ckpt'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Resuming training from {checkpoint_path}\")\n",
    "    trocr_trainer_s3.fit(trocr_model_full, trocr_dm, ckpt_path=checkpoint_path)\n",
    "else:\n",
    "    print(\"Starting training from scratch\")\n",
    "    trocr_trainer_s3.fit(trocr_model_full, trocr_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568ec39",
   "metadata": {},
   "source": [
    "## TrOCR Evaluation - Strategy 3\n",
    "\n",
    "Same as before, after training Strategy 3, we'll evaluate it on the test set to determine if full fine-tuning provides better performance or if the model overfits on the 800 receipts it got during training.\n",
    "- In the best case, we expect a val loss of approximately 0.15-0.4, token accuracy of 93%-96%.\n",
    "- If it overfits, the performance should be similar or worse than Strategy 2, since the model memorized training data.\n",
    "\n",
    "The evaluation process is the same. First, we load the best checkpoint and compute metrics on the 100-receipt test set, and then the results will guide our final recommendation that we'll compare agains the best Donut model, which will be tested in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd840a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "trocr_trainer_s3.test(trocr_model_full, trocr_dm, ckpt_path=trocr_checkpoint_s3.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dcd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logs_s1 = pd.read_csv('./trocr_logs/strategy1_frozen/version_0/metrics.csv')\n",
    "    logs_s2 = pd.read_csv('./trocr_logs/strategy2_partial/version_0/metrics.csv')\n",
    "    logs_s3 = pd.read_csv('./trocr_logs/strategy3_full/version_0/metrics.csv')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].plot(logs_s1['epoch'], logs_s1['val_loss'], label='Strategy 1: Frozen', marker='o')\n",
    "    axes[0].plot(logs_s2['epoch'], logs_s2['val_loss'], label='Strategy 2: Partial', marker='s')\n",
    "    axes[0].plot(logs_s3['epoch'], logs_s3['val_loss'], label='Strategy 3: Full', marker='^')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Validation Loss')\n",
    "    axes[0].set_title('TrOCR: Validation Loss Comparison')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(logs_s1['epoch'], logs_s1['val_cer'], label='Strategy 1: Frozen', marker='o')\n",
    "    axes[1].plot(logs_s2['epoch'], logs_s2['val_cer'], label='Strategy 2: Partial', marker='s')\n",
    "    axes[1].plot(logs_s3['epoch'], logs_s3['val_cer'], label='Strategy 3: Full', marker='^')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Character Error Rate (CER)')\n",
    "    axes[1].set_title('TrOCR: CER Comparison')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"TrOCR Strategy Comparison - Final Metrics\")\n",
    "    print(f\"Strategy 1 (Frozen) - Best Val Loss: {logs_s1['val_loss'].min():.4f}, Best CER: {logs_s1['val_cer'].min():.4f}\")\n",
    "    print(f\"Strategy 2 (Partial)    - Best Val Loss: {logs_s2['val_loss'].min():.4f}, Best CER: {logs_s2['val_cer'].min():.4f}\")\n",
    "    print(f\"Strategy 3 (Full)   - Best Val Loss: {logs_s3['val_loss'].min():.4f}, Best CER: {logs_s3['val_cer'].min():.4f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Training logs not found. Please train the models first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
