# TrOCR Totals - Gu√≠a de Entrenamiento

## Cambios Principales Realizados

### ‚úÖ **Problema identificado**: El modelo recib√≠a im√°genes completas de recibos

**Soluci√≥n**: Ahora se extraen y croppean las regiones espec√≠ficas donde est√° el total usando bounding boxes del dataset CORD.

### üéØ Mejoras Implementadas

1. **Extracci√≥n de Bounding Boxes**

   - Nueva funci√≥n `extract_total_info()` que busca el campo total en `valid_line` del JSON
   - Extrae coordenadas del quad y crea un bbox con margen de 10px
   - Fallback a `gt_parse.total.total_price` si no hay bbox disponible

2. **Crop de Im√°genes**

   - Se recorta solo la regi√≥n del total antes de pasarla al modelo
   - TrOCR ahora ve texto de l√≠nea √∫nica en lugar de documento completo
   - Validaci√≥n robusta de coordenadas del bbox

3. **Formato de Salida Simplificado**

   - Antes: `"TOTAL 45500 CASH 50000 CHANGE 4500"` (confuso)
   - Ahora: `"45500"` (solo el n√∫mero limpio)
   - M√°s f√°cil para el modelo aprender

4. **Augmentaci√≥n Mejorada**

   - ColorJitter m√°s agresivo (brightness=0.2, contrast=0.2)
   - GaussianBlur aleatorio (30% probabilidad)
   - Aplicado despu√©s del crop para mejor efecto

5. **Hiperpar√°metros Optimizados**
   - `max_length`: 64 ‚Üí 32 (suficiente para n√∫meros)
   - `learning_rate`: 3e-5 ‚Üí 5e-5 (aprende m√°s r√°pido)
   - `warmup_steps`: 500 ‚Üí 300 (warmup m√°s corto)
   - `unfreeze_last_n_layers`: 0 ‚Üí 2 (encoder m√°s adaptable)
   - `early_stopping patience`: 8 ‚Üí 5 (detiene antes si no mejora)

## üìä Comandos Recomendados

### Para M√°quina Potente (RTX 4090/A100):

```bash
# Entrenamiento completo con batch grande
python train_totals_trocr.py \
  --epochs 30 \
  --batch 16 \
  --lr 5e-5 \
  --num_workers 8
```

### Para GPU Mediana (RTX 3080/4070):

```bash
python train_totals_trocr.py \
  --epochs 30 \
  --batch 8 \
  --lr 5e-5 \
  --num_workers 4
```

### Para GPU Peque√±a (RTX 3060):

```bash
python train_totals_trocr.py \
  --epochs 30 \
  --batch 4 \
  --lr 4e-5 \
  --num_workers 2
```

## üîç Evaluaci√≥n

```bash
# Evaluar el mejor checkpoint
python evaluate_totals_trocr.py \
  --checkpoint trocr_checkpoints/totals/totals-epoch=XX-val_loss=Y.YYY.ckpt

# Ver ejemplos de predicciones
python preview_totals_predictions.py \
  --checkpoint trocr_checkpoints/totals/totals-epoch=XX-val_loss=Y.YYY.ckpt \
  --num 50
```

## üìà M√©tricas Esperadas

Con estos cambios, deber√≠as ver:

| M√©trica       | Valor Esperado | Explicaci√≥n                                        |
| ------------- | -------------- | -------------------------------------------------- |
| **MAE**       | < 5,000        | Error absoluto promedio en la predicci√≥n del total |
| **Median AE** | < 1,000        | El 50% de predicciones con error menor a esto      |
| **Train Acc** | > 0.90         | Accuracy a nivel de token durante entrenamiento    |
| **Val Acc**   | > 0.85         | Accuracy en validaci√≥n                             |

### ‚ö†Ô∏è Si los resultados a√∫n son malos:

1. **Verificar que se usan bboxes**: En el output deber√≠a decir cu√°ntos samples tienen bbox

   ```
   [CORDTotalsHFDataset] train: 800 v√°lidos de 800 (sin bbox: 50, sin texto: 0)
   ```

2. **Revisar predicciones**: Deben ser n√∫meros, no palabras:

   ```
   GT total_price: 45500
   Predicci√≥n completa: "45500"  ‚úÖ
   Predicci√≥n completa: "ITEM"   ‚ùå
   ```

3. **Incrementar √©pocas**: Si val_loss sigue bajando al final, necesita m√°s entrenamiento

4. **Probar descongelar m√°s capas**: Cambiar `unfreeze_last_n_layers` de 2 a 4 en el c√≥digo

5. **Usar modelo m√°s grande**: Cambiar de `trocr-base-printed` a `trocr-large-printed`

## üêõ Debugging

### Si el modelo predice palabras random:

- **Causa**: No se est√°n usando los crops correctamente
- **Soluci√≥n**: Verificar que `extract_total_info()` encuentra bboxes

### Si MAE es gigante (>100M):

- **Causa**: Overflow en conversi√≥n de strings a n√∫meros
- **Soluci√≥n**: Ya est√° arreglado con `clean_number()` m√°s robusto

### Si val_loss no baja de ~1.5:

- **Causa**: Modelo no aprende la tarea
- **Soluci√≥n**:
  - Verificar que los crops son correctos
  - Aumentar batch size si hay memoria
  - Probar learning rate m√°s alto (1e-4)

## üìÅ Estructura de Checkpoints

```
trocr_checkpoints/totals/
‚îú‚îÄ‚îÄ last.ckpt                              # √öltimo checkpoint (para resumir)
‚îú‚îÄ‚îÄ totals-epoch=04-val_loss=0.123.ckpt   # Mejor modelo
‚îú‚îÄ‚îÄ totals-epoch=05-val_loss=0.145.ckpt   # Top 2
‚îî‚îÄ‚îÄ totals-epoch=03-val_loss=0.156.ckpt   # Top 3
```

## üöÄ Tips para Optimizar Velocidad

1. **Usar m√°s workers**: `--num_workers 8` o m√°s si tienes CPU potente
2. **Precision 16-mixed**: Ya est√° activado autom√°ticamente en GPU
3. **Batch size grande**: Aprovecha toda la VRAM disponible
4. **Pin memory**: Ya est√° activado (`pin_memory=True`)
5. **Persistent workers**: Ya est√° activado si `num_workers > 0`

## üéì Pr√≥ximos Pasos

Una vez que este modelo funcione bien:

1. **Extender a otros campos**: Modificar para predecir tambi√©n `subtotal`, `tax`, `cashprice`, etc.
2. **Multi-task learning**: Un solo modelo que prediga m√∫ltiples campos
3. **Ensemble**: Combinar predicciones de m√∫ltiples checkpoints
4. **Post-processing**: Validar que el total sea consistente con suma de items
5. **Active learning**: Encontrar ejemplos dif√≠ciles y re-entrenar

## üìù Notas Importantes

- Los cambios son **retrocompatibles**: Si ya tienes checkpoints viejos, funcionar√°n pero dar√°n malos resultados porque fueron entrenados con im√°genes completas
- **Necesitas re-entrenar desde cero** con estos cambios para ver mejoras
- El dataset CORD tiene ~800 im√°genes de train, suficiente para fine-tuning
- Si algunos samples no tienen bbox, se usar√° la imagen completa (no ideal pero mejor que nada)
